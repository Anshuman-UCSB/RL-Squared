{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00ebabd",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "For the first stage of this project, using trained data from nexto 2v2 using our observation builders, we create a model to predict what Nexto would do given our inputs. This is essentially model distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c7a63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eb7cd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPKL(name):\n",
    "    with open(os.path.join(dir, name), 'rb') as f:\n",
    "        return [(x.astype('float32'), y.astype('float32'))for x,y in pickle.load(f)]\n",
    "def loadAll():\n",
    "    data = []\n",
    "    files = [x for x in os.listdir(dir) if not x.startswith('.')]\n",
    "    for f in tqdm(files):\n",
    "        data+=loadPKL(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848a5ff",
   "metadata": {},
   "source": [
    "# Pytorch section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52f514",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3b3f72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import SELU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Module\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0b7d16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextoDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        data = loadAll()\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for x,y in data:\n",
    "            self.X.append(x)\n",
    "            self.Y.append(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return [self.X[i], self.Y[i]]\n",
    "    def get_splits(self, n_test = .2):\n",
    "        test_size = round(n_test*len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8aea82",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2ba6ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    # define model elements\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input to first hidden layer\n",
    "        n_inputs = 119\n",
    "        \n",
    "        self.hidden1 = Linear(n_inputs, 256)\n",
    "        self.act1 = SELU()\n",
    "\n",
    "        self.hidden2 = Linear(256, 256)\n",
    "        self.act2 = SELU()\n",
    "\n",
    "        self.hidden3 = Linear(256, 128)\n",
    "        self.act3 = SELU()\n",
    "\n",
    "        self.hidden4 = Linear(128, 64)\n",
    "        self.act4 = SELU()\n",
    "        \n",
    "        # third hidden layer and output\n",
    "        self.hidden5 = Linear(64, 8)\n",
    "        self.act5 = Tanh()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        \n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        \n",
    "        X = self.hidden5(X)\n",
    "        X = self.act5(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c45224e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden1): Linear(in_features=119, out_features=256, bias=True)\n",
       "  (act1): SELU()\n",
       "  (hidden2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (act2): SELU()\n",
       "  (hidden3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act3): SELU()\n",
       "  (hidden4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (act4): SELU()\n",
       "  (hidden5): Linear(in_features=64, out_features=8, bias=True)\n",
       "  (act5): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54870e4c",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6dfaca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size):\n",
    "    # load the dataset\n",
    "    dataset = NextoDataset()\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "160591ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, valid_dl, model,epochs = 10):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    min_valid_loss = np.inf\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = Adam(model.parameters())\n",
    "    with tqdm(total=len(train_dl)) as bar:\n",
    "        for e in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            model.train()     # Optional when not using Model Specific layer\n",
    "            bar.reset()\n",
    "            for data, labels in train_dl:\n",
    "                bar.update(1)\n",
    "                if torch.cuda.is_available():\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                target = model(data)\n",
    "                loss = criterion(target,labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            model.eval()     # Optional when not using Model Specific layer\n",
    "            for data, labels in valid_dl:\n",
    "                if torch.cuda.is_available():\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                target = model(data)\n",
    "                loss = criterion(target,labels)\n",
    "                valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "            print(f'Epoch {e+1} \\tTraining Loss: {train_loss / len(train_dl)} \\t Validation Loss: {valid_loss / len(valid_dl)}')\n",
    "            if min_valid_loss > valid_loss:\n",
    "                print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "                min_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f'models/{valid_loss/len(valid_dl)*1000:.6f}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c2f6b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1bcf4",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7c83a77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b043ea4ab34414ba50a76348a582ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207407 51852\n",
      "ARE YOU SURE YOU WANT TO RESET THE MODEL? (yes/[no])no\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = prepare_data(batch_size=128)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "if input(\"ARE YOU SURE YOU WANT TO RESET THE MODEL? (yes/[no])\") == \"yes\":\n",
    "    model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd1483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25aad065453426a85c1c66dceb16574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tTraining Loss: 0.22966109505779284 \t Validation Loss: 0.006753238110706724\n",
      "Validation Loss Decreased(inf--->2.741815) \t Saving The Model\n",
      "Epoch 2 \tTraining Loss: 0.2279113517323811 \t Validation Loss: 0.007073589440049796\n",
      "Epoch 3 \tTraining Loss: 0.22634795332013788 \t Validation Loss: 0.006975950865909971\n",
      "Epoch 4 \tTraining Loss: 0.22506079303857943 \t Validation Loss: 0.0072006847470851955\n",
      "Epoch 5 \tTraining Loss: 0.2239544895936647 \t Validation Loss: 0.0071136102887797235\n",
      "Epoch 6 \tTraining Loss: 0.22289563697992026 \t Validation Loss: 0.0067927423369121085\n",
      "Epoch 7 \tTraining Loss: 0.22180260687533312 \t Validation Loss: 0.00716015388225687\n",
      "Epoch 8 \tTraining Loss: 0.22068534335517942 \t Validation Loss: 0.007265676124929794\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_model(train_dl, test_dl, model, epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
