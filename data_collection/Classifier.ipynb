{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89d0c4a",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "For the first stage of this project, using trained data from nexto 2v2 using our observation builders, we create a model to predict what Nexto would do given our inputs. This is essentially model distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc44b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9bd1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPKL(name):\n",
    "    with open(os.path.join(dir, name), 'rb') as f:\n",
    "        return [(x.astype('float32'), y.astype('float32'))for x,y in pickle.load(f)]\n",
    "def loadAll():\n",
    "    data = []\n",
    "    files = [x for x in os.listdir(dir) if not x.startswith('.')]\n",
    "    for f in tqdm(files):\n",
    "        data+=loadPKL(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f5739",
   "metadata": {},
   "source": [
    "# Pytorch section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60bfc1",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e831311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import SELU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Module\n",
    "from torch.nn import Dropout\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd2a7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextoDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        data = loadAll()\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for x,y in data:\n",
    "            self.X.append(x)\n",
    "            self.Y.append(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return [self.X[i], self.Y[i]]\n",
    "    def get_splits(self, n_test = .2):\n",
    "        test_size = round(n_test*len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2342089",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6afd20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    # define model elements\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input to first hidden layer\n",
    "        n_inputs = 119\n",
    "        \n",
    "        self.dropout1 = Dropout(0.2)\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        \n",
    "        self.hidden1 = Linear(n_inputs, 256)\n",
    "        self.act1 = SELU()\n",
    "\n",
    "        self.hidden2 = Linear(256, 256)\n",
    "        self.act2 = SELU()\n",
    "\n",
    "        self.hidden3 = Linear(256, 128)\n",
    "        self.act3 = SELU()\n",
    "\n",
    "        self.hidden4 = Linear(128, 64)\n",
    "        self.act4 = SELU()\n",
    "        \n",
    "        # third hidden layer and output\n",
    "        self.hidden5 = Linear(64, 8)\n",
    "        self.act5 = Tanh()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        \n",
    "        X = self.dropout1(X)\n",
    "        \n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        \n",
    "        X = self.dropout2(X)\n",
    "        \n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        \n",
    "        X = self.hidden5(X)\n",
    "        X = self.act5(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c1152fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (hidden1): Linear(in_features=119, out_features=256, bias=True)\n",
       "  (act1): SELU()\n",
       "  (hidden2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (act2): SELU()\n",
       "  (hidden3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act3): SELU()\n",
       "  (hidden4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (act4): SELU()\n",
       "  (hidden5): Linear(in_features=64, out_features=8, bias=True)\n",
       "  (act5): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7bec56",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a2acb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size):\n",
    "    # load the dataset\n",
    "    dataset = NextoDataset()\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50185a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, valid_dl, model,epochs = 10):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    min_valid_loss = np.inf\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = Adam(model.parameters())\n",
    "    with tqdm(total=len(train_dl)) as bar:\n",
    "        for e in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            model.train()     # Optional when not using Model Specific layer\n",
    "#             bar.reset()\n",
    "            for data, labels in train_dl:\n",
    "#                 bar.update(1)\n",
    "                if torch.cuda.is_available():\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                target = model(data)\n",
    "                loss = criterion(target,labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            model.eval()     # Optional when not using Model Specific layer\n",
    "            for data, labels in valid_dl:\n",
    "                if torch.cuda.is_available():\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                target = model(data)\n",
    "                loss = criterion(target,labels)\n",
    "                valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "            print(f'Epoch {e+1} \\tTraining Loss: {train_loss / len(train_dl)} \\t Validation Loss: {valid_loss / len(valid_dl)}')\n",
    "            if min_valid_loss > valid_loss:\n",
    "                print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "                min_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f'models/{valid_loss/len(valid_dl)*1000:.6f}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8676bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d6aba",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a80c7768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7ad9664d1341b18bacbf0850df67c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345145 86286\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = prepare_data(batch_size=128)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55c5be9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Net()\n\u001b[0;32m      2\u001b[0m paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m----> 3\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(paths)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "paths = [float(x[:-4]) for x in os.listdir('models') if not x.startswith('.')]\n",
    "best = min(paths)\n",
    "\n",
    "model.load_state_dict(torch.load(f'models/{best:.6f}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4738d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (hidden1): Linear(in_features=119, out_features=256, bias=True)\n",
       "  (act1): SELU()\n",
       "  (hidden2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (act2): SELU()\n",
       "  (hidden3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act3): SELU()\n",
       "  (hidden4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (act4): SELU()\n",
       "  (hidden5): Linear(in_features=64, out_features=8, bias=True)\n",
       "  (act5): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e166594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3818b9c886843e9a4754420810b7dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2697 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tTraining Loss: 0.2594438068151562 \t Validation Loss: 0.0063584986439457645\n",
      "Validation Loss Decreased(inf--->4.291987) \t Saving The Model\n",
      "Epoch 2 \tTraining Loss: 0.25402330611872154 \t Validation Loss: 0.007201846705542671\n",
      "Epoch 3 \tTraining Loss: 0.25169311951698087 \t Validation Loss: 0.006790733602311876\n",
      "Epoch 4 \tTraining Loss: 0.24978592566949684 \t Validation Loss: 0.006352458971518057\n",
      "Validation Loss Decreased(4.291987--->4.287910) \t Saving The Model\n",
      "Epoch 5 \tTraining Loss: 0.24845119293062254 \t Validation Loss: 0.007173640551390472\n",
      "Epoch 6 \tTraining Loss: 0.2470862954457159 \t Validation Loss: 0.006688327436093931\n",
      "Epoch 7 \tTraining Loss: 0.24585621176625252 \t Validation Loss: 0.006713459050213849\n",
      "Epoch 8 \tTraining Loss: 0.24481407304052338 \t Validation Loss: 0.006564212198610659\n",
      "Epoch 9 \tTraining Loss: 0.24386947556451644 \t Validation Loss: 0.006266007776613589\n",
      "Validation Loss Decreased(4.287910--->4.229555) \t Saving The Model\n",
      "Epoch 10 \tTraining Loss: 0.2431443692314479 \t Validation Loss: 0.006456607624336526\n",
      "Epoch 11 \tTraining Loss: 0.2423191313605243 \t Validation Loss: 0.006350492106543647\n",
      "Epoch 12 \tTraining Loss: 0.24169352218580548 \t Validation Loss: 0.0063955618717052316\n",
      "Epoch 13 \tTraining Loss: 0.2410048032010269 \t Validation Loss: 0.006258278157975939\n",
      "Validation Loss Decreased(4.229555--->4.224338) \t Saving The Model\n",
      "Epoch 14 \tTraining Loss: 0.2404694678355112 \t Validation Loss: 0.006676448980967204\n",
      "Epoch 15 \tTraining Loss: 0.2398435760691381 \t Validation Loss: 0.006138638743647822\n",
      "Validation Loss Decreased(4.224338--->4.143581) \t Saving The Model\n",
      "Epoch 16 \tTraining Loss: 0.2393213344539675 \t Validation Loss: 0.006069566673702664\n",
      "Validation Loss Decreased(4.143581--->4.096958) \t Saving The Model\n",
      "Epoch 17 \tTraining Loss: 0.23886066684026297 \t Validation Loss: 0.006348235960359927\n",
      "Epoch 18 \tTraining Loss: 0.23841990268601723 \t Validation Loss: 0.00588680523413199\n",
      "Validation Loss Decreased(4.096958--->3.973594) \t Saving The Model\n",
      "Epoch 19 \tTraining Loss: 0.2380443768707699 \t Validation Loss: 0.007046097207952429\n",
      "Epoch 20 \tTraining Loss: 0.2376269883730697 \t Validation Loss: 0.006423458964736373\n",
      "Epoch 21 \tTraining Loss: 0.2370427599771402 \t Validation Loss: 0.0066179121865166555\n",
      "Epoch 22 \tTraining Loss: 0.23670613134199042 \t Validation Loss: 0.006406492127312554\n",
      "Epoch 23 \tTraining Loss: 0.2363008557645135 \t Validation Loss: 0.006106097080089428\n",
      "Epoch 24 \tTraining Loss: 0.23587775991212273 \t Validation Loss: 0.006307853416160301\n",
      "Epoch 25 \tTraining Loss: 0.2354211065673722 \t Validation Loss: 0.005837069264164677\n",
      "Validation Loss Decreased(3.973594--->3.940022) \t Saving The Model\n",
      "Epoch 26 \tTraining Loss: 0.2351271305806468 \t Validation Loss: 0.006437696174339012\n",
      "Epoch 27 \tTraining Loss: 0.23465896841769843 \t Validation Loss: 0.006722710485811587\n",
      "Epoch 28 \tTraining Loss: 0.23458586975760842 \t Validation Loss: 0.006802117559644911\n",
      "Epoch 29 \tTraining Loss: 0.2340539664715186 \t Validation Loss: 0.006080700909649885\n",
      "Epoch 30 \tTraining Loss: 0.23376491695675622 \t Validation Loss: 0.006277501141583478\n",
      "Epoch 31 \tTraining Loss: 0.2333681515887971 \t Validation Loss: 0.006262465936166269\n",
      "Epoch 32 \tTraining Loss: 0.233131219788662 \t Validation Loss: 0.006396417352888319\n",
      "Epoch 33 \tTraining Loss: 0.2328855361779796 \t Validation Loss: 0.005965008205837673\n",
      "Epoch 34 \tTraining Loss: 0.23262383691306285 \t Validation Loss: 0.0064064290788438585\n",
      "Epoch 35 \tTraining Loss: 0.2322664808234684 \t Validation Loss: 0.006252738546442102\n",
      "Epoch 36 \tTraining Loss: 0.2319492512404322 \t Validation Loss: 0.006332905910633228\n",
      "Epoch 37 \tTraining Loss: 0.23179796808266312 \t Validation Loss: 0.005951109727223714\n",
      "Epoch 38 \tTraining Loss: 0.23149463448716306 \t Validation Loss: 0.006688966574492278\n",
      "Epoch 39 \tTraining Loss: 0.23125144273517836 \t Validation Loss: 0.0058195077931439436\n",
      "Validation Loss Decreased(3.940022--->3.928168) \t Saving The Model\n",
      "Epoch 40 \tTraining Loss: 0.2309466056915226 \t Validation Loss: 0.0060676208248844855\n",
      "Epoch 41 \tTraining Loss: 0.23062842404197045 \t Validation Loss: 0.005867783140253138\n",
      "Epoch 42 \tTraining Loss: 0.23047953003152105 \t Validation Loss: 0.0066015925230803315\n",
      "Epoch 43 \tTraining Loss: 0.2301737674528199 \t Validation Loss: 0.0062718780835469565\n",
      "Epoch 44 \tTraining Loss: 0.2300304760469257 \t Validation Loss: 0.005729853488780834\n",
      "Validation Loss Decreased(3.928168--->3.867651) \t Saving The Model\n",
      "Epoch 45 \tTraining Loss: 0.22966113952318296 \t Validation Loss: 0.006218856793862802\n",
      "Epoch 46 \tTraining Loss: 0.22938135234258508 \t Validation Loss: 0.0062573930069252295\n",
      "Epoch 47 \tTraining Loss: 0.22935403681791489 \t Validation Loss: 0.006104730411812111\n",
      "Epoch 48 \tTraining Loss: 0.22899673442112148 \t Validation Loss: 0.006001975624649613\n",
      "Epoch 49 \tTraining Loss: 0.2288684113946366 \t Validation Loss: 0.006071509431909631\n",
      "Epoch 50 \tTraining Loss: 0.2286283832193968 \t Validation Loss: 0.006384996926342999\n",
      "Epoch 51 \tTraining Loss: 0.22822774294729273 \t Validation Loss: 0.005640851303383156\n",
      "Validation Loss Decreased(3.867651--->3.807575) \t Saving The Model\n",
      "Epoch 52 \tTraining Loss: 0.22824152654902247 \t Validation Loss: 0.005932378768920899\n",
      "Epoch 53 \tTraining Loss: 0.22805590520954416 \t Validation Loss: 0.005684189708144576\n",
      "Epoch 54 \tTraining Loss: 0.227968629058706 \t Validation Loss: 0.005852722591824002\n",
      "Epoch 55 \tTraining Loss: 0.22767930645544 \t Validation Loss: 0.0061104171364395705\n",
      "Epoch 56 \tTraining Loss: 0.2275401766781105 \t Validation Loss: 0.005929831487161142\n",
      "Epoch 57 \tTraining Loss: 0.22721583051707156 \t Validation Loss: 0.005520651252181442\n",
      "Validation Loss Decreased(3.807575--->3.726440) \t Saving The Model\n",
      "Epoch 58 \tTraining Loss: 0.2270714877975133 \t Validation Loss: 0.005922063544944481\n",
      "Epoch 59 \tTraining Loss: 0.22698340462475827 \t Validation Loss: 0.005876449832209835\n",
      "Epoch 60 \tTraining Loss: 0.2268009116782672 \t Validation Loss: 0.005734394214771412\n",
      "Epoch 61 \tTraining Loss: 0.22658045310354427 \t Validation Loss: 0.006075529698972348\n",
      "Epoch 62 \tTraining Loss: 0.2265137172227088 \t Validation Loss: 0.006535181469387479\n",
      "Epoch 63 \tTraining Loss: 0.2263008160009002 \t Validation Loss: 0.006209479879449915\n",
      "Epoch 64 \tTraining Loss: 0.22610602237513475 \t Validation Loss: 0.006401421670560484\n",
      "Epoch 65 \tTraining Loss: 0.22590584531398805 \t Validation Loss: 0.006226057299861202\n",
      "Epoch 66 \tTraining Loss: 0.22581349894498337 \t Validation Loss: 0.005623462288467973\n",
      "Epoch 67 \tTraining Loss: 0.22576113551866137 \t Validation Loss: 0.005653834342956543\n",
      "Epoch 68 \tTraining Loss: 0.22553884742721433 \t Validation Loss: 0.0056543535656399194\n",
      "Epoch 69 \tTraining Loss: 0.22533281296657376 \t Validation Loss: 0.005715153305618851\n",
      "Epoch 70 \tTraining Loss: 0.225151218360647 \t Validation Loss: 0.005919752385881212\n",
      "Epoch 71 \tTraining Loss: 0.22496524012813668 \t Validation Loss: 0.005811805371884946\n",
      "Epoch 72 \tTraining Loss: 0.22492632580921745 \t Validation Loss: 0.005327038058528194\n",
      "Validation Loss Decreased(3.726440--->3.595751) \t Saving The Model\n",
      "Epoch 73 \tTraining Loss: 0.22486785007796112 \t Validation Loss: 0.006114512196293584\n",
      "Epoch 74 \tTraining Loss: 0.2245444419607658 \t Validation Loss: 0.005739924554471617\n",
      "Epoch 75 \tTraining Loss: 0.22444868676190383 \t Validation Loss: 0.00609996098059195\n",
      "Epoch 76 \tTraining Loss: 0.22438960528214597 \t Validation Loss: 0.005967848477540193\n",
      "Epoch 77 \tTraining Loss: 0.2243466434166703 \t Validation Loss: 0.005504497245506004\n",
      "Epoch 78 \tTraining Loss: 0.2240128058181854 \t Validation Loss: 0.006016325950622558\n",
      "Epoch 79 \tTraining Loss: 0.2239338398465944 \t Validation Loss: 0.0058340602450900605\n",
      "Epoch 80 \tTraining Loss: 0.2237566528848069 \t Validation Loss: 0.0061715617886296025\n",
      "Epoch 81 \tTraining Loss: 0.22392769032467016 \t Validation Loss: 0.005412514474656847\n",
      "Epoch 82 \tTraining Loss: 0.22374057049303972 \t Validation Loss: 0.005312631483431216\n",
      "Validation Loss Decreased(3.595751--->3.586026) \t Saving The Model\n",
      "Epoch 83 \tTraining Loss: 0.22335671363581452 \t Validation Loss: 0.006191255163263391\n",
      "Epoch 84 \tTraining Loss: 0.22324934231050375 \t Validation Loss: 0.005997023847368029\n",
      "Epoch 85 \tTraining Loss: 0.2232543426142121 \t Validation Loss: 0.00550416593198423\n",
      "Epoch 86 \tTraining Loss: 0.22320093208986783 \t Validation Loss: 0.005710053178999159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 \tTraining Loss: 0.22280522382586454 \t Validation Loss: 0.005935592386457655\n",
      "Epoch 88 \tTraining Loss: 0.22274352231642738 \t Validation Loss: 0.005199275281694201\n",
      "Validation Loss Decreased(3.586026--->3.509511) \t Saving The Model\n",
      "Epoch 89 \tTraining Loss: 0.2228217920109569 \t Validation Loss: 0.005712532467312283\n",
      "Epoch 90 \tTraining Loss: 0.2226800255734345 \t Validation Loss: 0.005443256784368444\n",
      "Epoch 91 \tTraining Loss: 0.22266982395780674 \t Validation Loss: 0.00553045528906363\n",
      "Epoch 92 \tTraining Loss: 0.2223323428568948 \t Validation Loss: 0.006056031650967068\n",
      "Epoch 93 \tTraining Loss: 0.22236619671495217 \t Validation Loss: 0.006067947193428322\n",
      "Epoch 94 \tTraining Loss: 0.22227629085448128 \t Validation Loss: 0.005265982415941026\n",
      "Epoch 95 \tTraining Loss: 0.2222099313426115 \t Validation Loss: 0.005745940738254123\n",
      "Epoch 96 \tTraining Loss: 0.22208116057960467 \t Validation Loss: 0.0055302989041363755\n",
      "Epoch 97 \tTraining Loss: 0.2218000528478251 \t Validation Loss: 0.005577346660472729\n",
      "Epoch 98 \tTraining Loss: 0.22188154629282833 \t Validation Loss: 0.00580531879707619\n",
      "Epoch 99 \tTraining Loss: 0.22173120800238608 \t Validation Loss: 0.005982227855258518\n",
      "Epoch 100 \tTraining Loss: 0.22164232341567808 \t Validation Loss: 0.006099458447209111\n",
      "Epoch 101 \tTraining Loss: 0.22150559897417488 \t Validation Loss: 0.005692016371974239\n",
      "Epoch 102 \tTraining Loss: 0.22150011432471434 \t Validation Loss: 0.005791556923477738\n",
      "Epoch 103 \tTraining Loss: 0.2213583979334352 \t Validation Loss: 0.005565296367362693\n",
      "Epoch 104 \tTraining Loss: 0.22123319799585345 \t Validation Loss: 0.006089374400951244\n",
      "Epoch 105 \tTraining Loss: 0.22118150583452148 \t Validation Loss: 0.0052945940582840535\n",
      "Epoch 106 \tTraining Loss: 0.22105472095926734 \t Validation Loss: 0.005572053679713497\n",
      "Epoch 107 \tTraining Loss: 0.22106635048405346 \t Validation Loss: 0.005779308831250226\n",
      "Epoch 108 \tTraining Loss: 0.22086463957019412 \t Validation Loss: 0.005567894335146303\n",
      "Epoch 109 \tTraining Loss: 0.220542266749434 \t Validation Loss: 0.005455079608493381\n",
      "Epoch 110 \tTraining Loss: 0.22065300737812557 \t Validation Loss: 0.005943318914484095\n",
      "Epoch 111 \tTraining Loss: 0.2206505187018783 \t Validation Loss: 0.005799502266777886\n",
      "Epoch 112 \tTraining Loss: 0.22048392290201813 \t Validation Loss: 0.005399867693583171\n",
      "Epoch 113 \tTraining Loss: 0.22056807991991584 \t Validation Loss: 0.0054361761940850154\n",
      "Epoch 114 \tTraining Loss: 0.22028046595550796 \t Validation Loss: 0.005606465781176532\n",
      "Epoch 115 \tTraining Loss: 0.22013140280002746 \t Validation Loss: 0.005594866099180999\n",
      "Epoch 116 \tTraining Loss: 0.2202271224007502 \t Validation Loss: 0.0058961994559676555\n",
      "Epoch 117 \tTraining Loss: 0.22016214791230904 \t Validation Loss: 0.005415757762061225\n",
      "Epoch 118 \tTraining Loss: 0.21998497119955898 \t Validation Loss: 0.005447124993359601\n",
      "Epoch 119 \tTraining Loss: 0.21999701858671852 \t Validation Loss: 0.005816012929987025\n",
      "Epoch 120 \tTraining Loss: 0.21993674438973024 \t Validation Loss: 0.006099202544600875\n",
      "Epoch 121 \tTraining Loss: 0.22009411885157576 \t Validation Loss: 0.005778334052474411\n",
      "Epoch 122 \tTraining Loss: 0.21988022397591708 \t Validation Loss: 0.0049473750149762186\n",
      "Validation Loss Decreased(3.509511--->3.339478) \t Saving The Model\n",
      "Epoch 123 \tTraining Loss: 0.2197180645230261 \t Validation Loss: 0.005490689012739394\n",
      "Epoch 124 \tTraining Loss: 0.21949815630780178 \t Validation Loss: 0.005206095024391457\n",
      "Epoch 125 \tTraining Loss: 0.21969325690190794 \t Validation Loss: 0.005530538735566316\n",
      "Epoch 126 \tTraining Loss: 0.21940036726895198 \t Validation Loss: 0.005432180651911983\n",
      "Epoch 127 \tTraining Loss: 0.2191787176566253 \t Validation Loss: 0.005816828233224374\n",
      "Epoch 128 \tTraining Loss: 0.21942321167175177 \t Validation Loss: 0.005491498134754322\n",
      "Epoch 129 \tTraining Loss: 0.21926054151844085 \t Validation Loss: 0.005148039747167516\n",
      "Epoch 130 \tTraining Loss: 0.21921634510529672 \t Validation Loss: 0.0054781578205249926\n",
      "Epoch 131 \tTraining Loss: 0.21912440359813443 \t Validation Loss: 0.0053194481355172615\n",
      "Epoch 132 \tTraining Loss: 0.21898976775576195 \t Validation Loss: 0.0056666066028453685\n",
      "Epoch 133 \tTraining Loss: 0.21895391538512676 \t Validation Loss: 0.0058899409682662395\n",
      "Epoch 134 \tTraining Loss: 0.2188971100812229 \t Validation Loss: 0.005670560730828179\n",
      "Epoch 135 \tTraining Loss: 0.21895917078765356 \t Validation Loss: 0.005997425008703161\n",
      "Epoch 136 \tTraining Loss: 0.21887870014133742 \t Validation Loss: 0.005958774443025942\n",
      "Epoch 137 \tTraining Loss: 0.2187262784066711 \t Validation Loss: 0.00581705199347602\n",
      "Epoch 138 \tTraining Loss: 0.21866385053054554 \t Validation Loss: 0.005898524213720251\n",
      "Epoch 139 \tTraining Loss: 0.2183532122112355 \t Validation Loss: 0.0058344601701807094\n",
      "Epoch 140 \tTraining Loss: 0.2184194917861382 \t Validation Loss: 0.0059897442217226385\n",
      "Epoch 141 \tTraining Loss: 0.21831206100536002 \t Validation Loss: 0.005936922585522687\n",
      "Epoch 142 \tTraining Loss: 0.21830892489935055 \t Validation Loss: 0.0057329669705143685\n",
      "Epoch 143 \tTraining Loss: 0.2182396647165031 \t Validation Loss: 0.005920635682565195\n",
      "Epoch 144 \tTraining Loss: 0.21823166842853134 \t Validation Loss: 0.006117519979123716\n",
      "Epoch 145 \tTraining Loss: 0.21807033468340342 \t Validation Loss: 0.005629718921802662\n",
      "Epoch 146 \tTraining Loss: 0.2181712972275805 \t Validation Loss: 0.005496088928646512\n",
      "Epoch 147 \tTraining Loss: 0.21806688353137524 \t Validation Loss: 0.005364912262669316\n",
      "Epoch 148 \tTraining Loss: 0.21812646134080027 \t Validation Loss: 0.005525658042342575\n",
      "Epoch 149 \tTraining Loss: 0.21798243552369015 \t Validation Loss: 0.005564902623494466\n",
      "Epoch 150 \tTraining Loss: 0.2180587953998727 \t Validation Loss: 0.005495820663593434\n",
      "Epoch 151 \tTraining Loss: 0.2179990786013446 \t Validation Loss: 0.005570732134359854\n",
      "Epoch 152 \tTraining Loss: 0.21776210351214834 \t Validation Loss: 0.006205208654756899\n",
      "Epoch 153 \tTraining Loss: 0.21764872837252294 \t Validation Loss: 0.005438844627804226\n",
      "Epoch 154 \tTraining Loss: 0.21778285989595159 \t Validation Loss: 0.005806315828252722\n",
      "Epoch 155 \tTraining Loss: 0.21754669642554508 \t Validation Loss: 0.005834020685266565\n",
      "Epoch 156 \tTraining Loss: 0.2176478895813525 \t Validation Loss: 0.005650021764967176\n",
      "Epoch 157 \tTraining Loss: 0.2176964084528356 \t Validation Loss: 0.005702934265136719\n",
      "Epoch 158 \tTraining Loss: 0.21748925570894445 \t Validation Loss: 0.005940060792145906\n",
      "Epoch 159 \tTraining Loss: 0.2172931905450492 \t Validation Loss: 0.005731970557460078\n",
      "Epoch 160 \tTraining Loss: 0.21747404568983 \t Validation Loss: 0.005779141320122613\n",
      "Epoch 161 \tTraining Loss: 0.21723186977492204 \t Validation Loss: 0.005988684142077411\n",
      "Epoch 162 \tTraining Loss: 0.2174678872614646 \t Validation Loss: 0.005401288138495552\n",
      "Epoch 163 \tTraining Loss: 0.2173603690572257 \t Validation Loss: 0.005565385376965558\n",
      "Epoch 164 \tTraining Loss: 0.2171373895839448 \t Validation Loss: 0.006005718972947862\n",
      "Epoch 165 \tTraining Loss: 0.2170580060096595 \t Validation Loss: 0.006233963701460097\n",
      "Epoch 166 \tTraining Loss: 0.2172864746905982 \t Validation Loss: 0.006005276397422508\n",
      "Epoch 167 \tTraining Loss: 0.21707050377433285 \t Validation Loss: 0.005848372865606237\n",
      "Epoch 168 \tTraining Loss: 0.2172247287318315 \t Validation Loss: 0.005727755581891095\n",
      "Epoch 169 \tTraining Loss: 0.2172319142132014 \t Validation Loss: 0.005605907616791902\n",
      "Epoch 170 \tTraining Loss: 0.2169457900499829 \t Validation Loss: 0.00577694018681844\n",
      "Epoch 171 \tTraining Loss: 0.21696276075428045 \t Validation Loss: 0.005908256548422354\n",
      "Epoch 172 \tTraining Loss: 0.21704807793006747 \t Validation Loss: 0.005310515032874213\n",
      "Epoch 173 \tTraining Loss: 0.21681757924907685 \t Validation Loss: 0.005583102614791305\n",
      "Epoch 174 \tTraining Loss: 0.21658157268563244 \t Validation Loss: 0.005712251839814363\n",
      "Epoch 175 \tTraining Loss: 0.21662041233851814 \t Validation Loss: 0.00516434457567003\n",
      "Epoch 176 \tTraining Loss: 0.2165991957591205 \t Validation Loss: 0.006007335362610993\n",
      "Epoch 177 \tTraining Loss: 0.21655706592763316 \t Validation Loss: 0.005718792809380425\n",
      "Epoch 178 \tTraining Loss: 0.2165567206649105 \t Validation Loss: 0.005868329560315168\n",
      "Epoch 179 \tTraining Loss: 0.2163848920048808 \t Validation Loss: 0.005694307133003517\n",
      "Epoch 180 \tTraining Loss: 0.21633375042405092 \t Validation Loss: 0.005282435593781648\n",
      "Epoch 181 \tTraining Loss: 0.21658905853286337 \t Validation Loss: 0.005661257991084346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 \tTraining Loss: 0.21617260260364501 \t Validation Loss: 0.005869694992348\n",
      "Epoch 183 \tTraining Loss: 0.21643355246225815 \t Validation Loss: 0.005592821968926324\n",
      "Epoch 184 \tTraining Loss: 0.21653371522790818 \t Validation Loss: 0.00580335502271299\n",
      "Epoch 185 \tTraining Loss: 0.21651750397452346 \t Validation Loss: 0.00647013955646091\n",
      "Epoch 186 \tTraining Loss: 0.21612127908720632 \t Validation Loss: 0.005781848077420835\n",
      "Epoch 187 \tTraining Loss: 0.21619293125726843 \t Validation Loss: 0.005519756829297101\n",
      "Epoch 188 \tTraining Loss: 0.21658709767375736 \t Validation Loss: 0.005549701143194128\n",
      "Epoch 189 \tTraining Loss: 0.2161024077748828 \t Validation Loss: 0.005489676528506808\n",
      "Epoch 190 \tTraining Loss: 0.21597313610743626 \t Validation Loss: 0.00608207314102738\n",
      "Epoch 191 \tTraining Loss: 0.21587186061677377 \t Validation Loss: 0.0053812041106047455\n",
      "Epoch 192 \tTraining Loss: 0.2159548663555979 \t Validation Loss: 0.005737411269435176\n",
      "Epoch 193 \tTraining Loss: 0.2161144750468325 \t Validation Loss: 0.005717565218607584\n",
      "Epoch 194 \tTraining Loss: 0.21574068654012274 \t Validation Loss: 0.0056024887826707625\n",
      "Epoch 195 \tTraining Loss: 0.21573047202894058 \t Validation Loss: 0.00580639989287765\n",
      "Epoch 196 \tTraining Loss: 0.21582730162528255 \t Validation Loss: 0.0062118349251923735\n",
      "Epoch 197 \tTraining Loss: 0.21607475105395438 \t Validation Loss: 0.005867771395930537\n",
      "Epoch 198 \tTraining Loss: 0.21573704678923014 \t Validation Loss: 0.005937442426328306\n",
      "Epoch 199 \tTraining Loss: 0.21588059561447961 \t Validation Loss: 0.00567248679973461\n",
      "Epoch 200 \tTraining Loss: 0.21577502389306885 \t Validation Loss: 0.005969516789471662\n",
      "Epoch 201 \tTraining Loss: 0.21567116500958275 \t Validation Loss: 0.005673763222164578\n",
      "Epoch 202 \tTraining Loss: 0.21557156010650025 \t Validation Loss: 0.00603968169954088\n",
      "Epoch 203 \tTraining Loss: 0.21532732955517483 \t Validation Loss: 0.00590051703982883\n",
      "Epoch 204 \tTraining Loss: 0.21563483895108926 \t Validation Loss: 0.005435445573594835\n",
      "Epoch 205 \tTraining Loss: 0.21564763777778465 \t Validation Loss: 0.0057970983893783\n",
      "Epoch 206 \tTraining Loss: 0.21547946199073056 \t Validation Loss: 0.006624665790134006\n",
      "Epoch 207 \tTraining Loss: 0.21516471826062186 \t Validation Loss: 0.006053106696517379\n",
      "Epoch 208 \tTraining Loss: 0.21558755963852902 \t Validation Loss: 0.0057583674678096065\n",
      "Epoch 209 \tTraining Loss: 0.2152989327708659 \t Validation Loss: 0.005468950271606445\n",
      "Epoch 210 \tTraining Loss: 0.2152944381176475 \t Validation Loss: 0.005849852650253861\n",
      "Epoch 211 \tTraining Loss: 0.2152009821729303 \t Validation Loss: 0.005654478426332827\n",
      "Epoch 212 \tTraining Loss: 0.21541359837204604 \t Validation Loss: 0.005870667916757089\n",
      "Epoch 213 \tTraining Loss: 0.2151590518136797 \t Validation Loss: 0.005914282622160735\n",
      "Epoch 214 \tTraining Loss: 0.21527683834897884 \t Validation Loss: 0.0057622097156665945\n",
      "Epoch 215 \tTraining Loss: 0.21510814361807412 \t Validation Loss: 0.006197272583290382\n",
      "Epoch 216 \tTraining Loss: 0.21534810601162124 \t Validation Loss: 0.00601618625499584\n",
      "Epoch 217 \tTraining Loss: 0.21498945424478938 \t Validation Loss: 0.006127754847208659\n",
      "Epoch 218 \tTraining Loss: 0.21493035031107563 \t Validation Loss: 0.005939762239102964\n",
      "Epoch 219 \tTraining Loss: 0.21492066757299744 \t Validation Loss: 0.005788937321415654\n",
      "Epoch 220 \tTraining Loss: 0.2147740518766109 \t Validation Loss: 0.006019687917497423\n",
      "Epoch 221 \tTraining Loss: 0.21519971017069317 \t Validation Loss: 0.006224282052781847\n",
      "Epoch 222 \tTraining Loss: 0.21485421988072995 \t Validation Loss: 0.0056859705183241105\n",
      "Epoch 223 \tTraining Loss: 0.2151690684438503 \t Validation Loss: 0.00551400829244543\n",
      "Epoch 224 \tTraining Loss: 0.2146996640865123 \t Validation Loss: 0.005555939850983797\n",
      "Epoch 225 \tTraining Loss: 0.21509345350970063 \t Validation Loss: 0.006467898863333243\n",
      "Epoch 226 \tTraining Loss: 0.21490890195425413 \t Validation Loss: 0.005898178683386908\n",
      "Epoch 227 \tTraining Loss: 0.21470834580027354 \t Validation Loss: 0.005722387808340567\n",
      "Epoch 228 \tTraining Loss: 0.21466914008271046 \t Validation Loss: 0.0057484824569137005\n",
      "Epoch 229 \tTraining Loss: 0.21501251974501873 \t Validation Loss: 0.005814087479202835\n",
      "Epoch 230 \tTraining Loss: 0.2150599250351008 \t Validation Loss: 0.005582666838610614\n",
      "Epoch 231 \tTraining Loss: 0.21452961157086076 \t Validation Loss: 0.00596547179751926\n",
      "Epoch 232 \tTraining Loss: 0.21488920530434844 \t Validation Loss: 0.005999126081113462\n",
      "Epoch 233 \tTraining Loss: 0.21491133231320203 \t Validation Loss: 0.0059265226787990994\n",
      "Epoch 234 \tTraining Loss: 0.2146982973455985 \t Validation Loss: 0.005954784463953089\n",
      "Epoch 235 \tTraining Loss: 0.2144276628814276 \t Validation Loss: 0.005738855821114999\n",
      "Epoch 236 \tTraining Loss: 0.21487276932355162 \t Validation Loss: 0.0058158478913483795\n",
      "Epoch 237 \tTraining Loss: 0.21462238721706975 \t Validation Loss: 0.005642809514646177\n",
      "Epoch 238 \tTraining Loss: 0.21471488334909827 \t Validation Loss: 0.006062776600873029\n",
      "Epoch 239 \tTraining Loss: 0.21456366369116073 \t Validation Loss: 0.005874607209806089\n",
      "Epoch 240 \tTraining Loss: 0.21444395377342462 \t Validation Loss: 0.0054822473172788265\n",
      "Epoch 241 \tTraining Loss: 0.21437941246289963 \t Validation Loss: 0.005527837541368273\n",
      "Epoch 242 \tTraining Loss: 0.21431884509371438 \t Validation Loss: 0.006101741172649242\n",
      "Epoch 243 \tTraining Loss: 0.2143289587223225 \t Validation Loss: 0.006070565559245922\n",
      "Epoch 244 \tTraining Loss: 0.21429965909450285 \t Validation Loss: 0.0061850022386621545\n",
      "Epoch 245 \tTraining Loss: 0.21436146531539976 \t Validation Loss: 0.006150085749449553\n",
      "Epoch 246 \tTraining Loss: 0.21416770302458166 \t Validation Loss: 0.00570957413426152\n",
      "Epoch 247 \tTraining Loss: 0.21429833722835034 \t Validation Loss: 0.005747870515894007\n",
      "Epoch 248 \tTraining Loss: 0.21416300058939478 \t Validation Loss: 0.005705469802573875\n",
      "Epoch 249 \tTraining Loss: 0.21427684258787022 \t Validation Loss: 0.005558697294305872\n",
      "Epoch 250 \tTraining Loss: 0.21463604648775378 \t Validation Loss: 0.005912565478572139\n",
      "Epoch 251 \tTraining Loss: 0.21416550909518134 \t Validation Loss: 0.005771168779443812\n",
      "Epoch 252 \tTraining Loss: 0.2138911444399593 \t Validation Loss: 0.005885153611501058\n",
      "Epoch 253 \tTraining Loss: 0.21413918184292594 \t Validation Loss: 0.005918688597502532\n",
      "Epoch 254 \tTraining Loss: 0.21389287199671728 \t Validation Loss: 0.005747247448673954\n",
      "Epoch 255 \tTraining Loss: 0.21408740368913093 \t Validation Loss: 0.00575917226296884\n",
      "Epoch 256 \tTraining Loss: 0.2140610227219741 \t Validation Loss: 0.00567539753737273\n",
      "Epoch 257 \tTraining Loss: 0.2140038697408843 \t Validation Loss: 0.0058927843305799695\n",
      "Epoch 258 \tTraining Loss: 0.21384920152770973 \t Validation Loss: 0.005462100240919325\n",
      "Epoch 259 \tTraining Loss: 0.21404729555348886 \t Validation Loss: 0.00579448558666088\n",
      "Epoch 260 \tTraining Loss: 0.21397465923825237 \t Validation Loss: 0.005722160339355469\n",
      "Epoch 261 \tTraining Loss: 0.21401898646889503 \t Validation Loss: 0.00551996699085942\n",
      "Epoch 262 \tTraining Loss: 0.21433902211130926 \t Validation Loss: 0.0057666045648080335\n",
      "Epoch 263 \tTraining Loss: 0.2138636940644766 \t Validation Loss: 0.005346359941694472\n",
      "Epoch 264 \tTraining Loss: 0.21373709634051397 \t Validation Loss: 0.005712467564476861\n",
      "Epoch 265 \tTraining Loss: 0.2140098087072461 \t Validation Loss: 0.005811734905949346\n",
      "Epoch 266 \tTraining Loss: 0.2140670067454251 \t Validation Loss: 0.0058979833567584005\n",
      "Epoch 267 \tTraining Loss: 0.21373981808153045 \t Validation Loss: 0.0054177147370797615\n",
      "Epoch 268 \tTraining Loss: 0.2137521920731919 \t Validation Loss: 0.005969021055433486\n",
      "Epoch 269 \tTraining Loss: 0.21378431407597315 \t Validation Loss: 0.006002406455852367\n",
      "Epoch 270 \tTraining Loss: 0.2139855980552653 \t Validation Loss: 0.005527812816478588\n",
      "Epoch 271 \tTraining Loss: 0.21358293579448626 \t Validation Loss: 0.005597506717399315\n",
      "Epoch 272 \tTraining Loss: 0.21371990957214163 \t Validation Loss: 0.006076558254383228\n",
      "Epoch 273 \tTraining Loss: 0.21342517281400747 \t Validation Loss: 0.0057789206504821776\n",
      "Epoch 274 \tTraining Loss: 0.21369652731311645 \t Validation Loss: 0.005783787745016592\n",
      "Epoch 275 \tTraining Loss: 0.21380458313047687 \t Validation Loss: 0.006006261066154197\n",
      "Epoch 276 \tTraining Loss: 0.21370094819536553 \t Validation Loss: 0.006148479867864538\n",
      "Epoch 277 \tTraining Loss: 0.21373026569131837 \t Validation Loss: 0.005734049302560312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 \tTraining Loss: 0.2133641489344612 \t Validation Loss: 0.005941880544026693\n",
      "Epoch 279 \tTraining Loss: 0.21357163775837948 \t Validation Loss: 0.0058289811346266\n",
      "Epoch 280 \tTraining Loss: 0.2139575784742103 \t Validation Loss: 0.006360960006713867\n",
      "Epoch 281 \tTraining Loss: 0.21347089901339977 \t Validation Loss: 0.005758455859290229\n",
      "Epoch 282 \tTraining Loss: 0.21361298357441463 \t Validation Loss: 0.00562068259274518\n",
      "Epoch 283 \tTraining Loss: 0.21347165608299984 \t Validation Loss: 0.005830219233477557\n",
      "Epoch 284 \tTraining Loss: 0.2136348159137725 \t Validation Loss: 0.005934748031474926\n",
      "Epoch 285 \tTraining Loss: 0.21348670307335518 \t Validation Loss: 0.005690726968977187\n",
      "Epoch 286 \tTraining Loss: 0.21338808555867347 \t Validation Loss: 0.0051392784825077765\n",
      "Epoch 287 \tTraining Loss: 0.2140557195108645 \t Validation Loss: 0.005479973863672327\n",
      "Epoch 288 \tTraining Loss: 0.21361725958886746 \t Validation Loss: 0.005526928901672363\n",
      "Epoch 289 \tTraining Loss: 0.21341278484939954 \t Validation Loss: 0.005508925473248517\n",
      "Epoch 290 \tTraining Loss: 0.2131316299720477 \t Validation Loss: 0.005834059008845576\n",
      "Epoch 291 \tTraining Loss: 0.21363427327282128 \t Validation Loss: 0.0055813094421669285\n",
      "Epoch 292 \tTraining Loss: 0.21315901425090772 \t Validation Loss: 0.005895962715148926\n",
      "Epoch 293 \tTraining Loss: 0.213362317141401 \t Validation Loss: 0.005728676584031847\n",
      "Epoch 294 \tTraining Loss: 0.21313931342956088 \t Validation Loss: 0.0058373622541074395\n",
      "Epoch 295 \tTraining Loss: 0.21327718817772226 \t Validation Loss: 0.005505023267534044\n",
      "Epoch 296 \tTraining Loss: 0.2133258259656653 \t Validation Loss: 0.005898622495156747\n",
      "Epoch 297 \tTraining Loss: 0.21317621595911684 \t Validation Loss: 0.005664140913221571\n",
      "Epoch 298 \tTraining Loss: 0.21334118912363212 \t Validation Loss: 0.005568053810684769\n",
      "Epoch 299 \tTraining Loss: 0.21315132206375614 \t Validation Loss: 0.005859170843053747\n",
      "Epoch 300 \tTraining Loss: 0.2130803359872021 \t Validation Loss: 0.005803353786468506\n",
      "Epoch 301 \tTraining Loss: 0.21317778608402943 \t Validation Loss: 0.005672675327018455\n",
      "Epoch 302 \tTraining Loss: 0.2130950299244613 \t Validation Loss: 0.005668808354271783\n",
      "Epoch 303 \tTraining Loss: 0.21321907912442806 \t Validation Loss: 0.005626973840925429\n",
      "Epoch 304 \tTraining Loss: 0.2131039461726651 \t Validation Loss: 0.006051485979998553\n",
      "Epoch 305 \tTraining Loss: 0.21323619332077506 \t Validation Loss: 0.006607291610152633\n",
      "Epoch 306 \tTraining Loss: 0.2134415241559877 \t Validation Loss: 0.005858819749620226\n",
      "Epoch 307 \tTraining Loss: 0.2132191159215113 \t Validation Loss: 0.0059291731869732895\n",
      "Epoch 308 \tTraining Loss: 0.21293869841682234 \t Validation Loss: 0.006113591812275074\n",
      "Epoch 309 \tTraining Loss: 0.2129908434640579 \t Validation Loss: 0.0057992142218130606\n",
      "Epoch 310 \tTraining Loss: 0.2133557733050266 \t Validation Loss: 0.005946572091844347\n",
      "Epoch 311 \tTraining Loss: 0.21266106151190783 \t Validation Loss: 0.00591826147503323\n",
      "Epoch 312 \tTraining Loss: 0.21270922361191089 \t Validation Loss: 0.0058907513265256525\n",
      "Epoch 313 \tTraining Loss: 0.21283683544359783 \t Validation Loss: 0.00594901923780088\n",
      "Epoch 314 \tTraining Loss: 0.21316511004135527 \t Validation Loss: 0.005912210676405165\n",
      "Epoch 315 \tTraining Loss: 0.21306394845037316 \t Validation Loss: 0.005635360523506447\n",
      "Epoch 316 \tTraining Loss: 0.21312467710238986 \t Validation Loss: 0.0056934980109885885\n",
      "Epoch 317 \tTraining Loss: 0.2129790141907806 \t Validation Loss: 0.005392723436708803\n",
      "Epoch 318 \tTraining Loss: 0.2131257627489481 \t Validation Loss: 0.006330762880819815\n",
      "Epoch 319 \tTraining Loss: 0.21295106512641837 \t Validation Loss: 0.005685337561148185\n",
      "Epoch 320 \tTraining Loss: 0.21283868964088817 \t Validation Loss: 0.005936517715454102\n",
      "Epoch 321 \tTraining Loss: 0.21365877428472949 \t Validation Loss: 0.005919814816227666\n",
      "Epoch 322 \tTraining Loss: 0.21262581135371814 \t Validation Loss: 0.006368816958533393\n",
      "Epoch 323 \tTraining Loss: 0.2128642586964256 \t Validation Loss: 0.005910353219067609\n",
      "Epoch 324 \tTraining Loss: 0.21338181080511423 \t Validation Loss: 0.00572833847116541\n",
      "Epoch 325 \tTraining Loss: 0.21297829859248787 \t Validation Loss: 0.00606517615141692\n",
      "Epoch 326 \tTraining Loss: 0.2124865304818187 \t Validation Loss: 0.006085649596320258\n",
      "Epoch 327 \tTraining Loss: 0.2127811099186092 \t Validation Loss: 0.005593597712340179\n",
      "Epoch 328 \tTraining Loss: 0.21305179741809577 \t Validation Loss: 0.005685506926642524\n",
      "Epoch 329 \tTraining Loss: 0.212685561573501 \t Validation Loss: 0.00542772028181288\n",
      "Epoch 330 \tTraining Loss: 0.21246126521769657 \t Validation Loss: 0.005534835921393501\n",
      "Epoch 331 \tTraining Loss: 0.21269006587904737 \t Validation Loss: 0.006207193445276331\n",
      "Epoch 332 \tTraining Loss: 0.21274863365826271 \t Validation Loss: 0.006022962111013907\n",
      "Epoch 333 \tTraining Loss: 0.2127540156902717 \t Validation Loss: 0.005971879252681026\n",
      "Epoch 334 \tTraining Loss: 0.21266883133759001 \t Validation Loss: 0.005831597027955232\n",
      "Epoch 335 \tTraining Loss: 0.21255953972389136 \t Validation Loss: 0.005669821456626609\n",
      "Epoch 336 \tTraining Loss: 0.21343632628067272 \t Validation Loss: 0.0057127339751632126\n",
      "Epoch 337 \tTraining Loss: 0.21245664328856073 \t Validation Loss: 0.005668469005160862\n",
      "Epoch 338 \tTraining Loss: 0.2124166241786548 \t Validation Loss: 0.0057375861980296945\n",
      "Epoch 339 \tTraining Loss: 0.21226870512515914 \t Validation Loss: 0.0057898737766124584\n",
      "Epoch 340 \tTraining Loss: 0.21235379660111867 \t Validation Loss: 0.005413422496230514\n",
      "Epoch 341 \tTraining Loss: 0.21238831680131304 \t Validation Loss: 0.005506260130140516\n",
      "Epoch 342 \tTraining Loss: 0.21289314683229782 \t Validation Loss: 0.0056330963417335796\n",
      "Epoch 343 \tTraining Loss: 0.21237278391818096 \t Validation Loss: 0.006117740648764151\n",
      "Epoch 344 \tTraining Loss: 0.21209821991779912 \t Validation Loss: 0.005648820135328505\n",
      "Epoch 345 \tTraining Loss: 0.21250702694892706 \t Validation Loss: 0.006073230902353922\n",
      "Epoch 346 \tTraining Loss: 0.21302728286419792 \t Validation Loss: 0.005737170819882994\n",
      "Epoch 347 \tTraining Loss: 0.21217527244106063 \t Validation Loss: 0.005637025126704463\n",
      "Epoch 348 \tTraining Loss: 0.21210000793559047 \t Validation Loss: 0.006136272571705006\n",
      "Epoch 349 \tTraining Loss: 0.21261081680749405 \t Validation Loss: 0.006243372758229573\n",
      "Epoch 350 \tTraining Loss: 0.2127078721589144 \t Validation Loss: 0.005446382628546821\n",
      "Epoch 351 \tTraining Loss: 0.2125552736380209 \t Validation Loss: 0.0056879194577535\n",
      "Epoch 352 \tTraining Loss: 0.21289809765884687 \t Validation Loss: 0.005544222107640019\n",
      "Epoch 353 \tTraining Loss: 0.2120696454190483 \t Validation Loss: 0.0053803226682874895\n",
      "Epoch 354 \tTraining Loss: 0.21240312765495045 \t Validation Loss: 0.0057315619786580405\n",
      "Epoch 355 \tTraining Loss: 0.21281583729301243 \t Validation Loss: 0.005460988857128002\n",
      "Epoch 356 \tTraining Loss: 0.21230288786405452 \t Validation Loss: 0.005992040545852096\n",
      "Epoch 357 \tTraining Loss: 0.2125531390701615 \t Validation Loss: 0.005493617675922535\n",
      "Epoch 358 \tTraining Loss: 0.212734492345788 \t Validation Loss: 0.006114190772727683\n",
      "Epoch 359 \tTraining Loss: 0.212338262419591 \t Validation Loss: 0.005966945400944463\n",
      "Epoch 360 \tTraining Loss: 0.21211625606340967 \t Validation Loss: 0.005721655333483661\n",
      "Epoch 361 \tTraining Loss: 0.2125999302633347 \t Validation Loss: 0.005290245568310773\n",
      "Epoch 362 \tTraining Loss: 0.21221116159220382 \t Validation Loss: 0.005956002164770055\n",
      "Epoch 363 \tTraining Loss: 0.21207627194388337 \t Validation Loss: 0.005779371261596679\n",
      "Epoch 364 \tTraining Loss: 0.2134001095968129 \t Validation Loss: 0.005644503169589573\n",
      "Epoch 365 \tTraining Loss: 0.21228221194210165 \t Validation Loss: 0.005496494416837339\n",
      "Epoch 366 \tTraining Loss: 0.21242137008523074 \t Validation Loss: 0.005408278482931631\n",
      "Epoch 367 \tTraining Loss: 0.21248619572228752 \t Validation Loss: 0.005463213479077375\n",
      "Epoch 368 \tTraining Loss: 0.2125213180796765 \t Validation Loss: 0.0061358052712899664\n",
      "Epoch 369 \tTraining Loss: 0.21209747618576044 \t Validation Loss: 0.005452149709065755\n",
      "Epoch 370 \tTraining Loss: 0.2122135493085258 \t Validation Loss: 0.00550044298171997\n",
      "Epoch 371 \tTraining Loss: 0.21245741609913532 \t Validation Loss: 0.005493851944252297\n",
      "Epoch 372 \tTraining Loss: 0.21205093239495876 \t Validation Loss: 0.005745835657472964\n",
      "Epoch 373 \tTraining Loss: 0.21244620901702022 \t Validation Loss: 0.005380895049483688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 \tTraining Loss: 0.21219980863120144 \t Validation Loss: 0.005484857647507279\n",
      "Epoch 375 \tTraining Loss: 0.21206107995312967 \t Validation Loss: 0.005746060035846851\n",
      "Epoch 376 \tTraining Loss: 0.21216396517805758 \t Validation Loss: 0.005425626083656594\n",
      "Epoch 377 \tTraining Loss: 0.21293093809429078 \t Validation Loss: 0.005358303299656621\n",
      "Epoch 378 \tTraining Loss: 0.2122385224944183 \t Validation Loss: 0.005876946184370253\n",
      "Epoch 379 \tTraining Loss: 0.21238317354148875 \t Validation Loss: 0.006024899924242938\n",
      "Epoch 380 \tTraining Loss: 0.21195134607274577 \t Validation Loss: 0.005353026390075684\n",
      "Epoch 381 \tTraining Loss: 0.21225769507319212 \t Validation Loss: 0.005306971338060167\n",
      "Epoch 382 \tTraining Loss: 0.21190759741822393 \t Validation Loss: 0.005661406958544696\n",
      "Epoch 383 \tTraining Loss: 0.21195385564487423 \t Validation Loss: 0.00579056112854569\n",
      "Epoch 384 \tTraining Loss: 0.2121852676891423 \t Validation Loss: 0.005458079355734366\n",
      "Epoch 385 \tTraining Loss: 0.21205497675906476 \t Validation Loss: 0.005878228788022642\n",
      "Epoch 386 \tTraining Loss: 0.2121206554862186 \t Validation Loss: 0.005642871326870388\n",
      "Epoch 387 \tTraining Loss: 0.21205303463553074 \t Validation Loss: 0.005786948822162769\n",
      "Epoch 388 \tTraining Loss: 0.2121803476041805 \t Validation Loss: 0.0057834644670839664\n",
      "Epoch 389 \tTraining Loss: 0.21228118140829197 \t Validation Loss: 0.005671171435603389\n",
      "Epoch 390 \tTraining Loss: 0.21333455533883197 \t Validation Loss: 0.005817279462461119\n",
      "Epoch 391 \tTraining Loss: 0.21222479929847102 \t Validation Loss: 0.0054198231520476165\n",
      "Epoch 392 \tTraining Loss: 0.21176572161711804 \t Validation Loss: 0.006094583935207791\n",
      "Epoch 393 \tTraining Loss: 0.21193322481868618 \t Validation Loss: 0.005224031077490913\n",
      "Epoch 394 \tTraining Loss: 0.21213961176886398 \t Validation Loss: 0.00604334345570317\n",
      "Epoch 395 \tTraining Loss: 0.21169818799651283 \t Validation Loss: 0.0056525715192159015\n",
      "Epoch 396 \tTraining Loss: 0.21212132554121446 \t Validation Loss: 0.005864914434927481\n",
      "Epoch 397 \tTraining Loss: 0.21156019827127898 \t Validation Loss: 0.005508232558215106\n",
      "Epoch 398 \tTraining Loss: 0.21198233120588184 \t Validation Loss: 0.005703925733213071\n",
      "Epoch 399 \tTraining Loss: 0.21205458380378614 \t Validation Loss: 0.006421560711330838\n",
      "Epoch 400 \tTraining Loss: 0.21188683460874383 \t Validation Loss: 0.0058095999117250795\n",
      "Epoch 401 \tTraining Loss: 0.21234316361427838 \t Validation Loss: 0.005675605226446081\n",
      "Epoch 402 \tTraining Loss: 0.21165195208016616 \t Validation Loss: 0.005518956360993562\n",
      "Epoch 403 \tTraining Loss: 0.21176714056458437 \t Validation Loss: 0.005708760685390896\n",
      "Epoch 404 \tTraining Loss: 0.21197227514868627 \t Validation Loss: 0.005704882586443865\n",
      "Epoch 405 \tTraining Loss: 0.2119105476216117 \t Validation Loss: 0.005434404037616871\n",
      "Epoch 406 \tTraining Loss: 0.2117085561161798 \t Validation Loss: 0.005700359785998309\n",
      "Epoch 407 \tTraining Loss: 0.21195272467602258 \t Validation Loss: 0.005320343176523844\n",
      "Epoch 408 \tTraining Loss: 0.2119750903134086 \t Validation Loss: 0.005678193304273817\n",
      "Epoch 409 \tTraining Loss: 0.21231292688740505 \t Validation Loss: 0.005507641015229402\n",
      "Epoch 410 \tTraining Loss: 0.211775554996709 \t Validation Loss: 0.005436152705439815\n",
      "Epoch 411 \tTraining Loss: 0.21205165592727726 \t Validation Loss: 0.005577930785991527\n",
      "Epoch 412 \tTraining Loss: 0.2116635496091701 \t Validation Loss: 0.005409484439425998\n",
      "Epoch 413 \tTraining Loss: 0.21177252854711265 \t Validation Loss: 0.005573271380530463\n",
      "Epoch 414 \tTraining Loss: 0.21185929112448532 \t Validation Loss: 0.005549749356729013\n",
      "Epoch 415 \tTraining Loss: 0.21165368447690086 \t Validation Loss: 0.005461985888304534\n",
      "Epoch 416 \tTraining Loss: 0.211727701970105 \t Validation Loss: 0.00529626175209328\n",
      "Epoch 417 \tTraining Loss: 0.21130190793014608 \t Validation Loss: 0.0056607437133789065\n",
      "Epoch 418 \tTraining Loss: 0.21167082672964257 \t Validation Loss: 0.005324671886585377\n",
      "Epoch 419 \tTraining Loss: 0.2121257401303888 \t Validation Loss: 0.005338041870682327\n",
      "Epoch 420 \tTraining Loss: 0.21196932722715967 \t Validation Loss: 0.005552861602218063\n",
      "Epoch 421 \tTraining Loss: 0.21160568172042354 \t Validation Loss: 0.006101647836190683\n",
      "Epoch 422 \tTraining Loss: 0.21221382907133524 \t Validation Loss: 0.00557987787105419\n",
      "Epoch 423 \tTraining Loss: 0.21178182627498815 \t Validation Loss: 0.005349627335866292\n",
      "Epoch 424 \tTraining Loss: 0.21154490475704107 \t Validation Loss: 0.006015199113775183\n",
      "Epoch 425 \tTraining Loss: 0.21133366401190223 \t Validation Loss: 0.005749127158412227\n",
      "Epoch 426 \tTraining Loss: 0.21160314628536012 \t Validation Loss: 0.005669498796816225\n",
      "Epoch 427 \tTraining Loss: 0.2122034627252214 \t Validation Loss: 0.005378581417931451\n",
      "Epoch 428 \tTraining Loss: 0.21142286356727769 \t Validation Loss: 0.00532681182578758\n",
      "Epoch 429 \tTraining Loss: 0.21141283209119677 \t Validation Loss: 0.00613736664807355\n",
      "Epoch 430 \tTraining Loss: 0.21197134961916597 \t Validation Loss: 0.0054255395465426974\n",
      "Epoch 431 \tTraining Loss: 0.21138077138926217 \t Validation Loss: 0.005933011107974583\n",
      "Epoch 432 \tTraining Loss: 0.21150148344738287 \t Validation Loss: 0.005541861498797381\n",
      "Epoch 433 \tTraining Loss: 0.21191071723075014 \t Validation Loss: 0.005592351577900074\n",
      "Epoch 434 \tTraining Loss: 0.21130073860613646 \t Validation Loss: 0.00569525471440068\n",
      "Epoch 435 \tTraining Loss: 0.21137546265859006 \t Validation Loss: 0.005517875265192103\n",
      "Epoch 436 \tTraining Loss: 0.2117410664828919 \t Validation Loss: 0.005806943840450711\n",
      "Epoch 437 \tTraining Loss: 0.21174506595013626 \t Validation Loss: 0.005261411401960585\n",
      "Epoch 438 \tTraining Loss: 0.21174460104716722 \t Validation Loss: 0.006008671124776204\n",
      "Epoch 439 \tTraining Loss: 0.21143613455138208 \t Validation Loss: 0.005693464632387514\n",
      "Epoch 440 \tTraining Loss: 0.21169374350003944 \t Validation Loss: 0.005281643161067256\n",
      "Epoch 441 \tTraining Loss: 0.21148855137767553 \t Validation Loss: 0.005677021962625009\n",
      "Epoch 442 \tTraining Loss: 0.21139491351944983 \t Validation Loss: 0.005749405313421179\n",
      "Epoch 443 \tTraining Loss: 0.2115711073854211 \t Validation Loss: 0.005702132560588696\n",
      "Epoch 444 \tTraining Loss: 0.21118154253259516 \t Validation Loss: 0.005458784015090377\n",
      "Epoch 445 \tTraining Loss: 0.21114706040025422 \t Validation Loss: 0.005825616077140525\n",
      "Epoch 446 \tTraining Loss: 0.21149857131737357 \t Validation Loss: 0.005474614125710947\n",
      "Epoch 447 \tTraining Loss: 0.21137035498585488 \t Validation Loss: 0.00563563311541522\n",
      "Epoch 448 \tTraining Loss: 0.211604660093055 \t Validation Loss: 0.005611321131388346\n",
      "Epoch 449 \tTraining Loss: 0.2122946888495694 \t Validation Loss: 0.005546771861888744\n",
      "Epoch 450 \tTraining Loss: 0.21149912375983018 \t Validation Loss: 0.005196388032701281\n",
      "Epoch 451 \tTraining Loss: 0.21174607164315218 \t Validation Loss: 0.0055050053419890225\n",
      "Epoch 452 \tTraining Loss: 0.2109645415853031 \t Validation Loss: 0.005900713602701823\n",
      "Epoch 453 \tTraining Loss: 0.21186639254717285 \t Validation Loss: 0.0058675575256347656\n",
      "Epoch 454 \tTraining Loss: 0.21144165048941357 \t Validation Loss: 0.005687950363865605\n",
      "Epoch 455 \tTraining Loss: 0.2113933436431662 \t Validation Loss: 0.005583155773304127\n",
      "Epoch 456 \tTraining Loss: 0.21109302320390178 \t Validation Loss: 0.005225587509296558\n",
      "Epoch 457 \tTraining Loss: 0.21129415628200202 \t Validation Loss: 0.005264717737833659\n",
      "Epoch 458 \tTraining Loss: 0.21154675280490892 \t Validation Loss: 0.005586241439536766\n",
      "Epoch 459 \tTraining Loss: 0.21100214456336164 \t Validation Loss: 0.0061767126012731485\n",
      "Epoch 460 \tTraining Loss: 0.21115293419268294 \t Validation Loss: 0.005587893062167697\n",
      "Epoch 461 \tTraining Loss: 0.21132087246727227 \t Validation Loss: 0.006182351730487964\n",
      "Epoch 462 \tTraining Loss: 0.21151585844343665 \t Validation Loss: 0.0059642244268346715\n",
      "Epoch 463 \tTraining Loss: 0.2113700399729867 \t Validation Loss: 0.006212344876042119\n",
      "Epoch 464 \tTraining Loss: 0.2116342589540927 \t Validation Loss: 0.005838515670211227\n",
      "Epoch 465 \tTraining Loss: 0.21180296830990422 \t Validation Loss: 0.005666499049575241\n",
      "Epoch 466 \tTraining Loss: 0.21132129434638436 \t Validation Loss: 0.0054526862391719114\n",
      "Epoch 467 \tTraining Loss: 0.21169829957013137 \t Validation Loss: 0.00573357829341182\n",
      "Epoch 468 \tTraining Loss: 0.21426963863060747 \t Validation Loss: 0.00546189996931288\n",
      "Epoch 469 \tTraining Loss: 0.21160644867440528 \t Validation Loss: 0.005406669510735406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 \tTraining Loss: 0.2114697547369106 \t Validation Loss: 0.0055615264398080335\n",
      "Epoch 471 \tTraining Loss: 0.21147024969083095 \t Validation Loss: 0.005649124251471626\n",
      "Epoch 472 \tTraining Loss: 0.21146143648551047 \t Validation Loss: 0.006110883818732368\n",
      "Epoch 473 \tTraining Loss: 0.2111009944300763 \t Validation Loss: 0.0056568742681432655\n",
      "Epoch 474 \tTraining Loss: 0.21113127435208429 \t Validation Loss: 0.006109002254627369\n",
      "Epoch 475 \tTraining Loss: 0.21149423548279755 \t Validation Loss: 0.005869077488228127\n",
      "Epoch 476 \tTraining Loss: 0.2115605406664812 \t Validation Loss: 0.005488821047323722\n",
      "Epoch 477 \tTraining Loss: 0.21085295537510845 \t Validation Loss: 0.006019838121202257\n",
      "Epoch 478 \tTraining Loss: 0.21114590938085975 \t Validation Loss: 0.005841683546702067\n",
      "Epoch 479 \tTraining Loss: 0.21130198592780738 \t Validation Loss: 0.0062023121339303475\n",
      "Epoch 480 \tTraining Loss: 0.2110249961266749 \t Validation Loss: 0.0057652273884526\n",
      "Epoch 481 \tTraining Loss: 0.21140988505920924 \t Validation Loss: 0.005710686754297327\n",
      "Epoch 482 \tTraining Loss: 0.2109720097356785 \t Validation Loss: 0.005468400760933205\n",
      "Epoch 483 \tTraining Loss: 0.2110220615371846 \t Validation Loss: 0.005958055566858363\n",
      "Epoch 484 \tTraining Loss: 0.210915755912866 \t Validation Loss: 0.005710517388802987\n",
      "Epoch 485 \tTraining Loss: 0.21129611374317914 \t Validation Loss: 0.005860537511331064\n",
      "Epoch 486 \tTraining Loss: 0.21108654241430172 \t Validation Loss: 0.005771562523312039\n",
      "Epoch 487 \tTraining Loss: 0.21131051003292042 \t Validation Loss: 0.005676983020923756\n",
      "Epoch 488 \tTraining Loss: 0.2113863175826732 \t Validation Loss: 0.005856347260651765\n",
      "Epoch 489 \tTraining Loss: 0.21105838446847855 \t Validation Loss: 0.0062367872838620785\n",
      "Epoch 490 \tTraining Loss: 0.2110800356291646 \t Validation Loss: 0.005822228149131492\n",
      "Epoch 491 \tTraining Loss: 0.21150209795314823 \t Validation Loss: 0.005683732297685411\n",
      "Epoch 492 \tTraining Loss: 0.21129044277671535 \t Validation Loss: 0.005946496062808566\n",
      "Epoch 493 \tTraining Loss: 0.21085386982682816 \t Validation Loss: 0.005479967682449906\n",
      "Epoch 494 \tTraining Loss: 0.21088732118778067 \t Validation Loss: 0.005864693765287046\n",
      "Epoch 495 \tTraining Loss: 0.2108534981873374 \t Validation Loss: 0.005853087902069092\n",
      "Epoch 496 \tTraining Loss: 0.21044862238862225 \t Validation Loss: 0.005352753798166911\n",
      "Epoch 497 \tTraining Loss: 0.21262237846762946 \t Validation Loss: 0.005753908333954988\n",
      "Epoch 498 \tTraining Loss: 0.21101575925763907 \t Validation Loss: 0.005953816484521936\n",
      "Epoch 499 \tTraining Loss: 0.21087602652750237 \t Validation Loss: 0.0058530786302354604\n",
      "Epoch 500 \tTraining Loss: 0.21217661502629154 \t Validation Loss: 0.005708258770130299\n",
      "Epoch 501 \tTraining Loss: 0.21154534410648007 \t Validation Loss: 0.006027630170186361\n",
      "Epoch 502 \tTraining Loss: 0.21290886786676222 \t Validation Loss: 0.0064835849514714\n",
      "Epoch 503 \tTraining Loss: 0.21087552250138822 \t Validation Loss: 0.005946263648845531\n",
      "Epoch 504 \tTraining Loss: 0.21073388533986495 \t Validation Loss: 0.006052967000890661\n",
      "Epoch 505 \tTraining Loss: 0.21102704666347738 \t Validation Loss: 0.006024982752623381\n",
      "Epoch 506 \tTraining Loss: 0.21151185258919106 \t Validation Loss: 0.006144197516971164\n",
      "Epoch 507 \tTraining Loss: 0.21140264263628675 \t Validation Loss: 0.006057188775804308\n",
      "Epoch 508 \tTraining Loss: 0.2115736608377951 \t Validation Loss: 0.005555254353417291\n",
      "Epoch 509 \tTraining Loss: 0.21079637728490608 \t Validation Loss: 0.005765428278181288\n",
      "Epoch 510 \tTraining Loss: 0.2115135578301203 \t Validation Loss: 0.005553658979910391\n",
      "Epoch 511 \tTraining Loss: 0.21059479724379615 \t Validation Loss: 0.005555329146208586\n",
      "Epoch 512 \tTraining Loss: 0.21080881195307927 \t Validation Loss: 0.005579405625661214\n",
      "Epoch 513 \tTraining Loss: 0.21191056957278465 \t Validation Loss: 0.006063250082510489\n",
      "Epoch 514 \tTraining Loss: 0.21113590157977907 \t Validation Loss: 0.00625703078729135\n",
      "Epoch 515 \tTraining Loss: 0.2111640484257013 \t Validation Loss: 0.00574014769660102\n",
      "Epoch 516 \tTraining Loss: 0.21111557525774263 \t Validation Loss: 0.006246371887348316\n",
      "Epoch 517 \tTraining Loss: 0.2104780466679427 \t Validation Loss: 0.005889986709312157\n",
      "Epoch 518 \tTraining Loss: 0.2108962418505294 \t Validation Loss: 0.005873332023620606\n",
      "Epoch 519 \tTraining Loss: 0.21114340140548157 \t Validation Loss: 0.0053361405266655814\n",
      "Epoch 520 \tTraining Loss: 0.2107414299187768 \t Validation Loss: 0.005970402558644613\n",
      "Epoch 521 \tTraining Loss: 0.21156188052231883 \t Validation Loss: 0.005260211626688639\n",
      "Epoch 522 \tTraining Loss: 0.21188021318356462 \t Validation Loss: 0.005910343947233977\n",
      "Epoch 523 \tTraining Loss: 0.21165887086555876 \t Validation Loss: 0.005956925021277534\n",
      "Epoch 524 \tTraining Loss: 0.21054049581013037 \t Validation Loss: 0.005579088528951009\n",
      "Epoch 525 \tTraining Loss: 0.21431795342745585 \t Validation Loss: 0.0058269264962938095\n",
      "Epoch 526 \tTraining Loss: 0.2127403830560349 \t Validation Loss: 0.005470768787242748\n",
      "Epoch 527 \tTraining Loss: 0.2105201237790444 \t Validation Loss: 0.005676065727516457\n",
      "Epoch 528 \tTraining Loss: 0.2111037096293891 \t Validation Loss: 0.006097942193349202\n",
      "Epoch 529 \tTraining Loss: 0.2111426968076382 \t Validation Loss: 0.00545838718061094\n",
      "Epoch 530 \tTraining Loss: 0.210987751594198 \t Validation Loss: 0.005543133594371655\n",
      "Epoch 531 \tTraining Loss: 0.21062034209945002 \t Validation Loss: 0.00582776528817636\n",
      "Epoch 532 \tTraining Loss: 0.2105825325715175 \t Validation Loss: 0.005940458862869828\n",
      "Epoch 533 \tTraining Loss: 0.21077590911729627 \t Validation Loss: 0.005905048493985776\n",
      "Epoch 534 \tTraining Loss: 0.21123711130532416 \t Validation Loss: 0.006292805230176007\n",
      "Epoch 535 \tTraining Loss: 0.21105267855031073 \t Validation Loss: 0.005926941765679253\n",
      "Epoch 536 \tTraining Loss: 0.21457810933098423 \t Validation Loss: 0.006178979255534984\n",
      "Epoch 537 \tTraining Loss: 0.21175230133962225 \t Validation Loss: 0.005469224717881945\n",
      "Epoch 538 \tTraining Loss: 0.21086738638450006 \t Validation Loss: 0.005585831624490244\n",
      "Epoch 539 \tTraining Loss: 0.21050148823325793 \t Validation Loss: 0.0061583098658808955\n",
      "Epoch 540 \tTraining Loss: 0.21033531280416093 \t Validation Loss: 0.005884498401924416\n",
      "Epoch 541 \tTraining Loss: 0.21034102381645947 \t Validation Loss: 0.005534229543473985\n",
      "Epoch 542 \tTraining Loss: 0.2118408608909532 \t Validation Loss: 0.005684579743279351\n",
      "Epoch 543 \tTraining Loss: 0.21091253564319745 \t Validation Loss: 0.005810799068874783\n",
      "Epoch 544 \tTraining Loss: 0.21066698096043365 \t Validation Loss: 0.005812838254151521\n",
      "Epoch 545 \tTraining Loss: 0.21232883317253082 \t Validation Loss: 0.005783398946126302\n",
      "Epoch 546 \tTraining Loss: 0.21069629526841097 \t Validation Loss: 0.006068122740145083\n",
      "Epoch 547 \tTraining Loss: 0.21090841714346634 \t Validation Loss: 0.0055472132011696144\n",
      "Epoch 548 \tTraining Loss: 0.2105267414858397 \t Validation Loss: 0.0056744437747531466\n",
      "Epoch 549 \tTraining Loss: 0.2105446857216184 \t Validation Loss: 0.005847467934643781\n",
      "Epoch 550 \tTraining Loss: 0.21144726742604594 \t Validation Loss: 0.005405600159256546\n",
      "Epoch 551 \tTraining Loss: 0.21046895610875097 \t Validation Loss: 0.005432293150160048\n",
      "Epoch 552 \tTraining Loss: 0.21055008843403017 \t Validation Loss: 0.0059921901314346875\n",
      "Epoch 553 \tTraining Loss: 0.21064286635657528 \t Validation Loss: 0.006278466648525662\n",
      "Epoch 554 \tTraining Loss: 0.21122262300629946 \t Validation Loss: 0.0055870925938641585\n",
      "Epoch 555 \tTraining Loss: 0.21080662748815396 \t Validation Loss: 0.005889148535551848\n",
      "Epoch 556 \tTraining Loss: 0.21120209880325147 \t Validation Loss: 0.005853360493977865\n",
      "Epoch 557 \tTraining Loss: 0.21037173267864145 \t Validation Loss: 0.005521288536213063\n",
      "Epoch 558 \tTraining Loss: 0.2106634393350964 \t Validation Loss: 0.005653535789913601\n",
      "Epoch 559 \tTraining Loss: 0.2104609328196762 \t Validation Loss: 0.00551656237354985\n",
      "Epoch 560 \tTraining Loss: 0.211166573026068 \t Validation Loss: 0.005987230318563956\n",
      "Epoch 561 \tTraining Loss: 0.21056363381608398 \t Validation Loss: 0.0059597646748578105\n",
      "Epoch 562 \tTraining Loss: 0.21028349085852177 \t Validation Loss: 0.0061782881948683\n",
      "Epoch 563 \tTraining Loss: 0.21092665659019935 \t Validation Loss: 0.006581755744086371\n",
      "Epoch 564 \tTraining Loss: 0.21060247664589504 \t Validation Loss: 0.005997411410013835\n",
      "Epoch 565 \tTraining Loss: 0.21118244868563332 \t Validation Loss: 0.0059376723678023725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566 \tTraining Loss: 0.2104875252650851 \t Validation Loss: 0.00585927097885697\n",
      "Epoch 567 \tTraining Loss: 0.21081605534841716 \t Validation Loss: 0.00585175461239285\n",
      "Epoch 568 \tTraining Loss: 0.2106252406586706 \t Validation Loss: 0.007071437747390182\n",
      "Epoch 569 \tTraining Loss: 0.21081520718669822 \t Validation Loss: 0.0061087630413196705\n",
      "Epoch 570 \tTraining Loss: 0.21123834352140564 \t Validation Loss: 0.005689276236074942\n",
      "Epoch 571 \tTraining Loss: 0.21082662568076435 \t Validation Loss: 0.005675036553983335\n",
      "Epoch 572 \tTraining Loss: 0.21040691063498673 \t Validation Loss: 0.006062769801528365\n",
      "Epoch 573 \tTraining Loss: 0.21041906902668076 \t Validation Loss: 0.005846739168520327\n",
      "Epoch 574 \tTraining Loss: 0.21081170198273827 \t Validation Loss: 0.005804416338602702\n",
      "Epoch 575 \tTraining Loss: 0.21076374894652403 \t Validation Loss: 0.006297594441307916\n",
      "Epoch 576 \tTraining Loss: 0.21071155473705924 \t Validation Loss: 0.005400239185050682\n",
      "Epoch 577 \tTraining Loss: 0.21069688991701158 \t Validation Loss: 0.0059775035469620316\n",
      "Epoch 578 \tTraining Loss: 0.21073897694012036 \t Validation Loss: 0.005968953061986852\n",
      "Epoch 579 \tTraining Loss: 0.2124288925195916 \t Validation Loss: 0.005493769115871854\n",
      "Epoch 580 \tTraining Loss: 0.21078484127775227 \t Validation Loss: 0.005597418325918692\n",
      "Epoch 581 \tTraining Loss: 0.21048230998574427 \t Validation Loss: 0.005771576122001365\n",
      "Epoch 582 \tTraining Loss: 0.21058979969710653 \t Validation Loss: 0.0060834713335390445\n",
      "Epoch 583 \tTraining Loss: 0.21026101540449507 \t Validation Loss: 0.005516342940153899\n",
      "Epoch 584 \tTraining Loss: 0.2105643603098494 \t Validation Loss: 0.006361841449031123\n",
      "Epoch 585 \tTraining Loss: 0.21087516489009484 \t Validation Loss: 0.005973053066818802\n",
      "Epoch 586 \tTraining Loss: 0.21058336486518495 \t Validation Loss: 0.005803903297141746\n",
      "Epoch 587 \tTraining Loss: 0.21053266792130285 \t Validation Loss: 0.005582928922441271\n",
      "Epoch 588 \tTraining Loss: 0.21105636051839752 \t Validation Loss: 0.0056654884197093825\n",
      "Epoch 589 \tTraining Loss: 0.21078747044040133 \t Validation Loss: 0.005969987180497911\n",
      "Epoch 590 \tTraining Loss: 0.2109445818583524 \t Validation Loss: 0.0057666991375110764\n",
      "Epoch 591 \tTraining Loss: 0.21034710248466063 \t Validation Loss: 0.005729429456922743\n",
      "Epoch 592 \tTraining Loss: 0.21277624254298289 \t Validation Loss: 0.005642884925559715\n",
      "Epoch 593 \tTraining Loss: 0.2103481263938903 \t Validation Loss: 0.00558777994579739\n",
      "Epoch 594 \tTraining Loss: 0.21004513434825406 \t Validation Loss: 0.006014406681060791\n",
      "Epoch 595 \tTraining Loss: 0.21237622446756607 \t Validation Loss: 0.005626246311046459\n",
      "Epoch 596 \tTraining Loss: 0.2109726831057269 \t Validation Loss: 0.005474469485106292\n",
      "Epoch 597 \tTraining Loss: 0.21075038806924476 \t Validation Loss: 0.006092660338790328\n",
      "Epoch 598 \tTraining Loss: 0.21062281067542868 \t Validation Loss: 0.006138444653263799\n",
      "Epoch 599 \tTraining Loss: 0.2106675440110584 \t Validation Loss: 0.005738703763043439\n",
      "Epoch 600 \tTraining Loss: 0.21202923206099325 \t Validation Loss: 0.005629390080769857\n",
      "Epoch 601 \tTraining Loss: 0.2110250536373115 \t Validation Loss: 0.005725705888536241\n",
      "Epoch 602 \tTraining Loss: 0.21059759254582863 \t Validation Loss: 0.0058564622313887985\n",
      "Epoch 603 \tTraining Loss: 0.21110791357459957 \t Validation Loss: 0.005313559284916631\n",
      "Epoch 604 \tTraining Loss: 0.21064691822927528 \t Validation Loss: 0.005594905659004494\n",
      "Epoch 605 \tTraining Loss: 0.21043733339708381 \t Validation Loss: 0.005537265759927255\n",
      "Epoch 606 \tTraining Loss: 0.2110986335822552 \t Validation Loss: 0.005669415968435782\n",
      "Epoch 607 \tTraining Loss: 0.21138696771420326 \t Validation Loss: 0.005793214109208849\n",
      "Epoch 608 \tTraining Loss: 0.2108100569210629 \t Validation Loss: 0.005684407905296043\n",
      "Epoch 609 \tTraining Loss: 0.21084830182037134 \t Validation Loss: 0.005889516936408149\n",
      "Epoch 610 \tTraining Loss: 0.21080121202936516 \t Validation Loss: 0.005886713752040157\n",
      "Epoch 611 \tTraining Loss: 0.21163189691837778 \t Validation Loss: 0.005811644041979754\n",
      "Epoch 612 \tTraining Loss: 0.21065400265679343 \t Validation Loss: 0.005537360950752541\n",
      "Epoch 613 \tTraining Loss: 0.21036649430995966 \t Validation Loss: 0.006063970194922553\n",
      "Epoch 614 \tTraining Loss: 0.21089173272710665 \t Validation Loss: 0.005907924616778339\n",
      "Epoch 615 \tTraining Loss: 0.21077543904837034 \t Validation Loss: 0.0059738244833769626\n",
      "Epoch 616 \tTraining Loss: 0.21148209176021104 \t Validation Loss: 0.006395626774540654\n",
      "Epoch 617 \tTraining Loss: 0.21117470630858623 \t Validation Loss: 0.0056996223661634655\n",
      "Epoch 618 \tTraining Loss: 0.2132727475874858 \t Validation Loss: 0.005427433473092538\n",
      "Epoch 619 \tTraining Loss: 0.2108781071373565 \t Validation Loss: 0.00622099240620931\n",
      "Epoch 620 \tTraining Loss: 0.21099883940072248 \t Validation Loss: 0.0060955444971720375\n",
      "Epoch 621 \tTraining Loss: 0.2137260593040192 \t Validation Loss: 0.0058531058276141135\n",
      "Epoch 622 \tTraining Loss: 0.2108412056575672 \t Validation Loss: 0.005712379172996238\n",
      "Epoch 623 \tTraining Loss: 0.21117155936788 \t Validation Loss: 0.005686819818284777\n",
      "Epoch 624 \tTraining Loss: 0.21061496929986237 \t Validation Loss: 0.005685217027310972\n",
      "Epoch 625 \tTraining Loss: 0.21118641500787733 \t Validation Loss: 0.005485617319742839\n",
      "Epoch 626 \tTraining Loss: 0.2114409852964948 \t Validation Loss: 0.005405215687221951\n",
      "Epoch 627 \tTraining Loss: 0.21061997380816233 \t Validation Loss: 0.00572343490741871\n",
      "Epoch 628 \tTraining Loss: 0.21153560903477236 \t Validation Loss: 0.005877229902479384\n",
      "Epoch 629 \tTraining Loss: 0.21122524709139306 \t Validation Loss: 0.005743572093822338\n",
      "Epoch 630 \tTraining Loss: 0.21076920082116507 \t Validation Loss: 0.005736799328415482\n",
      "Epoch 631 \tTraining Loss: 0.2107653332101357 \t Validation Loss: 0.006198296811845567\n",
      "Epoch 632 \tTraining Loss: 0.2120508299156344 \t Validation Loss: 0.005879848268296984\n",
      "Epoch 633 \tTraining Loss: 0.21102173381661501 \t Validation Loss: 0.005903405525066235\n",
      "Epoch 634 \tTraining Loss: 0.2110124789689442 \t Validation Loss: 0.006021513850600631\n",
      "Epoch 635 \tTraining Loss: 0.21127622051065748 \t Validation Loss: 0.006351223345156069\n",
      "Epoch 636 \tTraining Loss: 0.21072823736329055 \t Validation Loss: 0.0056254582051877625\n",
      "Epoch 637 \tTraining Loss: 0.21134179578607154 \t Validation Loss: 0.005661554689760561\n",
      "Epoch 638 \tTraining Loss: 0.21049074182741986 \t Validation Loss: 0.005799452198876275\n",
      "Epoch 639 \tTraining Loss: 0.21059482838871482 \t Validation Loss: 0.005579136124363652\n",
      "Epoch 640 \tTraining Loss: 0.21055375022449713 \t Validation Loss: 0.006010468624256276\n",
      "Epoch 641 \tTraining Loss: 0.21076064454899215 \t Validation Loss: 0.005753528188776087\n",
      "Epoch 642 \tTraining Loss: 0.2105268334122468 \t Validation Loss: 0.005749661834151656\n",
      "Epoch 643 \tTraining Loss: 0.21072509499183176 \t Validation Loss: 0.005643669940807201\n",
      "Epoch 644 \tTraining Loss: 0.21016922484272535 \t Validation Loss: 0.006049051814609104\n",
      "Epoch 645 \tTraining Loss: 0.2102066471283198 \t Validation Loss: 0.005690768383167408\n",
      "Epoch 646 \tTraining Loss: 0.21115789851702094 \t Validation Loss: 0.005883317788441976\n",
      "Epoch 647 \tTraining Loss: 0.21085260586911853 \t Validation Loss: 0.006005635526445177\n",
      "Epoch 648 \tTraining Loss: 0.21070656343924368 \t Validation Loss: 0.005840784178839789\n",
      "Epoch 649 \tTraining Loss: 0.2103164898363537 \t Validation Loss: 0.005794223502830223\n",
      "Epoch 650 \tTraining Loss: 0.21066612564925136 \t Validation Loss: 0.005657655574657299\n",
      "Epoch 651 \tTraining Loss: 0.21151136849206775 \t Validation Loss: 0.005690172513326009\n",
      "Epoch 652 \tTraining Loss: 0.21150005511831788 \t Validation Loss: 0.005723962783813477\n",
      "Epoch 653 \tTraining Loss: 0.21011130176940404 \t Validation Loss: 0.0060502923859490285\n",
      "Epoch 654 \tTraining Loss: 0.2105497556745807 \t Validation Loss: 0.00564681680114181\n",
      "Epoch 655 \tTraining Loss: 0.21069105994957749 \t Validation Loss: 0.006316179522761593\n",
      "Epoch 656 \tTraining Loss: 0.21294198790478805 \t Validation Loss: 0.005607525242699517\n",
      "Epoch 657 \tTraining Loss: 0.2109451997786219 \t Validation Loss: 0.005620015638845938\n",
      "Epoch 658 \tTraining Loss: 0.21002500352039133 \t Validation Loss: 0.006123554088451244\n",
      "Epoch 659 \tTraining Loss: 0.21036283948662815 \t Validation Loss: 0.0062532868208708585\n",
      "Epoch 660 \tTraining Loss: 0.21032644944209544 \t Validation Loss: 0.006056178145938449\n",
      "Epoch 661 \tTraining Loss: 0.2111846747876186 \t Validation Loss: 0.006379906071556939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 662 \tTraining Loss: 0.21094502311947172 \t Validation Loss: 0.005694498132776331\n",
      "Epoch 663 \tTraining Loss: 0.21276234216300213 \t Validation Loss: 0.00583719845171328\n",
      "Epoch 664 \tTraining Loss: 0.21203005306138167 \t Validation Loss: 0.005985128702940764\n",
      "Epoch 665 \tTraining Loss: 0.21062662595835357 \t Validation Loss: 0.005771128601498074\n",
      "Epoch 666 \tTraining Loss: 0.21140740314566916 \t Validation Loss: 0.006046065047935203\n",
      "Epoch 667 \tTraining Loss: 0.20999245959982238 \t Validation Loss: 0.005867114950109411\n",
      "Epoch 668 \tTraining Loss: 0.21134382686351555 \t Validation Loss: 0.00563866933186849\n",
      "Epoch 669 \tTraining Loss: 0.21064061585580326 \t Validation Loss: 0.005811728106604682\n",
      "Epoch 670 \tTraining Loss: 0.21077907266641574 \t Validation Loss: 0.005924431571254024\n",
      "Epoch 671 \tTraining Loss: 0.21120195448243886 \t Validation Loss: 0.005890352637679489\n",
      "Epoch 672 \tTraining Loss: 0.21071314307266048 \t Validation Loss: 0.00637682905903569\n",
      "Epoch 673 \tTraining Loss: 0.21039492686921063 \t Validation Loss: 0.0059166772277266895\n",
      "Epoch 674 \tTraining Loss: 0.21102451709605696 \t Validation Loss: 0.005790682898627387\n",
      "Epoch 675 \tTraining Loss: 0.21166138737167214 \t Validation Loss: 0.005527624289194743\n",
      "Epoch 676 \tTraining Loss: 0.21072724962680925 \t Validation Loss: 0.005767499605814616\n",
      "Epoch 677 \tTraining Loss: 0.21060448302619758 \t Validation Loss: 0.005895876796157272\n",
      "Epoch 678 \tTraining Loss: 0.21091073373533595 \t Validation Loss: 0.006350600896058259\n",
      "Epoch 679 \tTraining Loss: 0.2108856246157138 \t Validation Loss: 0.005457163916693793\n",
      "Epoch 680 \tTraining Loss: 0.21325896075357983 \t Validation Loss: 0.00566095820179692\n",
      "Epoch 681 \tTraining Loss: 0.21057431666131457 \t Validation Loss: 0.005570188186786793\n",
      "Epoch 682 \tTraining Loss: 0.2109808061668331 \t Validation Loss: 0.005398735293635616\n",
      "Epoch 683 \tTraining Loss: 0.2123636122893792 \t Validation Loss: 0.006514389691529451\n",
      "Epoch 684 \tTraining Loss: 0.21118964024459957 \t Validation Loss: 0.00599054839875963\n",
      "Epoch 685 \tTraining Loss: 0.21073516624854324 \t Validation Loss: 0.005439392902232982\n",
      "Epoch 686 \tTraining Loss: 0.21057372491891002 \t Validation Loss: 0.005782841399863914\n",
      "Epoch 687 \tTraining Loss: 0.21085814063664848 \t Validation Loss: 0.006192179256015354\n",
      "Epoch 688 \tTraining Loss: 0.2104475014257042 \t Validation Loss: 0.005511297826413755\n",
      "Epoch 689 \tTraining Loss: 0.21115217947124507 \t Validation Loss: 0.006264069345262316\n",
      "Epoch 690 \tTraining Loss: 0.21111046000458727 \t Validation Loss: 0.005921527014838324\n",
      "Epoch 691 \tTraining Loss: 0.2102285852620546 \t Validation Loss: 0.006549394572222675\n",
      "Epoch 692 \tTraining Loss: 0.21049106413340365 \t Validation Loss: 0.005979819651003237\n",
      "Epoch 693 \tTraining Loss: 0.21077477945236528 \t Validation Loss: 0.005951725995099103\n",
      "Epoch 694 \tTraining Loss: 0.2115152474461019 \t Validation Loss: 0.0061638921278494374\n",
      "Epoch 695 \tTraining Loss: 0.21282942256619852 \t Validation Loss: 0.005336197393911856\n",
      "Epoch 696 \tTraining Loss: 0.2111942473554505 \t Validation Loss: 0.005627162368209274\n",
      "Epoch 697 \tTraining Loss: 0.21071274675812862 \t Validation Loss: 0.006233761575486925\n",
      "Epoch 698 \tTraining Loss: 0.21212588223011616 \t Validation Loss: 0.0060046422040020975\n",
      "Epoch 699 \tTraining Loss: 0.21303237351966336 \t Validation Loss: 0.00557919979095459\n",
      "Epoch 700 \tTraining Loss: 0.211304578343099 \t Validation Loss: 0.005929938422309028\n",
      "Epoch 701 \tTraining Loss: 0.21110736020392826 \t Validation Loss: 0.005727329077544036\n",
      "Epoch 702 \tTraining Loss: 0.21175390263155386 \t Validation Loss: 0.005823286992532236\n",
      "Epoch 703 \tTraining Loss: 0.21278488571373322 \t Validation Loss: 0.006031831547066017\n",
      "Epoch 704 \tTraining Loss: 0.21125816976224046 \t Validation Loss: 0.005903193509137189\n",
      "Epoch 705 \tTraining Loss: 0.21147796617179257 \t Validation Loss: 0.005987823097794144\n",
      "Epoch 706 \tTraining Loss: 0.21170363161667302 \t Validation Loss: 0.00615211875350387\n",
      "Epoch 707 \tTraining Loss: 0.2113329885202202 \t Validation Loss: 0.005590544806586372\n",
      "Epoch 708 \tTraining Loss: 0.21197858306934625 \t Validation Loss: 0.005764176580641005\n",
      "Epoch 709 \tTraining Loss: 0.2145538468868325 \t Validation Loss: 0.005722253057691786\n",
      "Epoch 710 \tTraining Loss: 0.2113598933375054 \t Validation Loss: 0.0061989619113780835\n",
      "Epoch 711 \tTraining Loss: 0.21123436285050923 \t Validation Loss: 0.00588004297680325\n",
      "Epoch 712 \tTraining Loss: 0.21238752175226094 \t Validation Loss: 0.006142211490207248\n",
      "Epoch 713 \tTraining Loss: 0.2111959168711635 \t Validation Loss: 0.006262917783525255\n",
      "Epoch 714 \tTraining Loss: 0.2108766080926956 \t Validation Loss: 0.006284883993643301\n",
      "Epoch 715 \tTraining Loss: 0.21086079280576223 \t Validation Loss: 0.0058811797036065\n",
      "Epoch 716 \tTraining Loss: 0.21100575522466813 \t Validation Loss: 0.006119029433638961\n",
      "Epoch 717 \tTraining Loss: 0.21208729265374435 \t Validation Loss: 0.0063046218730785225\n",
      "Epoch 718 \tTraining Loss: 0.21090915178121086 \t Validation Loss: 0.006437487249021177\n",
      "Epoch 719 \tTraining Loss: 0.2119548807972486 \t Validation Loss: 0.005521253921367504\n",
      "Epoch 720 \tTraining Loss: 0.21189019525431244 \t Validation Loss: 0.005988558663262262\n",
      "Epoch 721 \tTraining Loss: 0.21155700926255597 \t Validation Loss: 0.005937444898817274\n",
      "Epoch 722 \tTraining Loss: 0.21251418837406566 \t Validation Loss: 0.006221588276050709\n",
      "Epoch 723 \tTraining Loss: 0.2128998662007045 \t Validation Loss: 0.006100287967258029\n",
      "Epoch 724 \tTraining Loss: 0.21166393290107058 \t Validation Loss: 0.0059209255818967466\n",
      "Epoch 725 \tTraining Loss: 0.2112229383009471 \t Validation Loss: 0.00594570795694987\n",
      "Epoch 726 \tTraining Loss: 0.21163316668295445 \t Validation Loss: 0.005379878238395409\n",
      "Epoch 727 \tTraining Loss: 0.21177702567004522 \t Validation Loss: 0.005637560420566135\n",
      "Epoch 728 \tTraining Loss: 0.2113044506472749 \t Validation Loss: 0.00643283587914926\n",
      "Epoch 729 \tTraining Loss: 0.21266589842772635 \t Validation Loss: 0.005957634625611482\n",
      "Epoch 730 \tTraining Loss: 0.21158227353768744 \t Validation Loss: 0.005675323980825919\n",
      "Epoch 731 \tTraining Loss: 0.21136274762537288 \t Validation Loss: 0.005903698515008997\n",
      "Epoch 732 \tTraining Loss: 0.2140944409158259 \t Validation Loss: 0.006179491060751456\n",
      "Epoch 733 \tTraining Loss: 0.21157286691586089 \t Validation Loss: 0.005948999457889133\n",
      "Epoch 734 \tTraining Loss: 0.21218615752107176 \t Validation Loss: 0.005745544521896928\n",
      "Epoch 735 \tTraining Loss: 0.2112449370504177 \t Validation Loss: 0.005839071362106888\n",
      "Epoch 736 \tTraining Loss: 0.21141137294871834 \t Validation Loss: 0.005578918545334427\n",
      "Epoch 737 \tTraining Loss: 0.21278695812435738 \t Validation Loss: 0.005834025630244502\n",
      "Epoch 738 \tTraining Loss: 0.21250668503993256 \t Validation Loss: 0.0059858098736515755\n",
      "Epoch 739 \tTraining Loss: 0.21121039704830946 \t Validation Loss: 0.006081732555671975\n",
      "Epoch 740 \tTraining Loss: 0.21291596870931556 \t Validation Loss: 0.006121639763867414\n",
      "Epoch 741 \tTraining Loss: 0.21085213413161616 \t Validation Loss: 0.005698899163140191\n",
      "Epoch 742 \tTraining Loss: 0.21093790877912943 \t Validation Loss: 0.005360259038430673\n",
      "Epoch 743 \tTraining Loss: 0.2143984845016548 \t Validation Loss: 0.005372635081962303\n",
      "Epoch 744 \tTraining Loss: 0.2121763451368136 \t Validation Loss: 0.005851214373553241\n",
      "Epoch 745 \tTraining Loss: 0.2111760455345639 \t Validation Loss: 0.005869016294126158\n",
      "Epoch 746 \tTraining Loss: 0.2135820521948556 \t Validation Loss: 0.006096749835544162\n",
      "Epoch 747 \tTraining Loss: 0.2128396146676255 \t Validation Loss: 0.005058895702715273\n",
      "Epoch 748 \tTraining Loss: 0.2118274558626991 \t Validation Loss: 0.006504671573638916\n",
      "Epoch 749 \tTraining Loss: 0.21139785161184566 \t Validation Loss: 0.005649728775024414\n",
      "Epoch 750 \tTraining Loss: 0.21119118683263907 \t Validation Loss: 0.005743698190759729\n",
      "Epoch 751 \tTraining Loss: 0.21168262217280862 \t Validation Loss: 0.006098640671482793\n",
      "Epoch 752 \tTraining Loss: 0.21142496598465246 \t Validation Loss: 0.0064398052074291084\n",
      "Epoch 753 \tTraining Loss: 0.21168232571318452 \t Validation Loss: 0.006242626684683341\n",
      "Epoch 754 \tTraining Loss: 0.21255507334254609 \t Validation Loss: 0.0060470027393764916\n",
      "Epoch 755 \tTraining Loss: 0.21386718008533248 \t Validation Loss: 0.006216218030011212\n",
      "Epoch 756 \tTraining Loss: 0.21134963114481747 \t Validation Loss: 0.006279546508082637\n",
      "Epoch 757 \tTraining Loss: 0.21254002402060376 \t Validation Loss: 0.0057537253697713215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 758 \tTraining Loss: 0.2116313391773622 \t Validation Loss: 0.00542369630601671\n",
      "Epoch 759 \tTraining Loss: 0.21138876015206465 \t Validation Loss: 0.005941112836201985\n",
      "Epoch 760 \tTraining Loss: 0.21231753237940534 \t Validation Loss: 0.005968241603286178\n",
      "Epoch 761 \tTraining Loss: 0.21124286730288575 \t Validation Loss: 0.006057615280151367\n",
      "Epoch 762 \tTraining Loss: 0.21397120250036064 \t Validation Loss: 0.0056798208201373065\n",
      "Epoch 763 \tTraining Loss: 0.2137120984022291 \t Validation Loss: 0.006155205037858751\n",
      "Epoch 764 \tTraining Loss: 0.21218701010838587 \t Validation Loss: 0.006298132825780798\n",
      "Epoch 765 \tTraining Loss: 0.2120003850041472 \t Validation Loss: 0.0064812193976508245\n",
      "Epoch 766 \tTraining Loss: 0.21176835686288853 \t Validation Loss: 0.006633173624674479\n",
      "Epoch 767 \tTraining Loss: 0.21178585602523753 \t Validation Loss: 0.006868056367944788\n",
      "Epoch 768 \tTraining Loss: 0.21142578871439333 \t Validation Loss: 0.006610781528331615\n",
      "Epoch 769 \tTraining Loss: 0.21226014796344714 \t Validation Loss: 0.005791267642268428\n",
      "Epoch 770 \tTraining Loss: 0.21366417866019263 \t Validation Loss: 0.006235912640889485\n",
      "Epoch 771 \tTraining Loss: 0.21198139627818757 \t Validation Loss: 0.0063207968959101924\n",
      "Epoch 772 \tTraining Loss: 0.2123080983531444 \t Validation Loss: 0.005656636909202293\n",
      "Epoch 773 \tTraining Loss: 0.21192096101937136 \t Validation Loss: 0.006326632587998001\n",
      "Epoch 774 \tTraining Loss: 0.21238941999002434 \t Validation Loss: 0.006166038866396303\n",
      "Epoch 775 \tTraining Loss: 0.21165172305423594 \t Validation Loss: 0.006269062536734122\n",
      "Epoch 776 \tTraining Loss: 0.2114640453434627 \t Validation Loss: 0.0063580066186410406\n",
      "Epoch 777 \tTraining Loss: 0.21141985058784485 \t Validation Loss: 0.006225652429792616\n",
      "Epoch 778 \tTraining Loss: 0.2116921505123145 \t Validation Loss: 0.006491883860694037\n",
      "Epoch 779 \tTraining Loss: 0.21247510763950864 \t Validation Loss: 0.005631387233734131\n",
      "Epoch 780 \tTraining Loss: 0.21385150795894328 \t Validation Loss: 0.006335338839778193\n",
      "Epoch 781 \tTraining Loss: 0.21132359297066916 \t Validation Loss: 0.005691209722448278\n",
      "Epoch 782 \tTraining Loss: 0.21152522693663384 \t Validation Loss: 0.006380664507548014\n",
      "Epoch 783 \tTraining Loss: 0.21125597794894868 \t Validation Loss: 0.006360886450167055\n",
      "Epoch 784 \tTraining Loss: 0.2124196184888167 \t Validation Loss: 0.006031512595989086\n",
      "Epoch 785 \tTraining Loss: 0.21287087602198987 \t Validation Loss: 0.005870883641419587\n",
      "Epoch 786 \tTraining Loss: 0.21208148493031284 \t Validation Loss: 0.00645723440029003\n",
      "Epoch 787 \tTraining Loss: 0.21151976308360293 \t Validation Loss: 0.006583000024159749\n",
      "Epoch 788 \tTraining Loss: 0.2123151861012402 \t Validation Loss: 0.006331727151517515\n",
      "Epoch 789 \tTraining Loss: 0.21250078001970885 \t Validation Loss: 0.006196711946416784\n",
      "Epoch 790 \tTraining Loss: 0.21373727781754048 \t Validation Loss: 0.0061013925517046895\n",
      "Epoch 791 \tTraining Loss: 0.21239984101792994 \t Validation Loss: 0.006021693724173087\n",
      "Epoch 792 \tTraining Loss: 0.21180221792566012 \t Validation Loss: 0.006315260993109809\n",
      "Epoch 793 \tTraining Loss: 0.2122478811474345 \t Validation Loss: 0.0060946797441553185\n",
      "Epoch 794 \tTraining Loss: 0.21375176226005405 \t Validation Loss: 0.006242690969396521\n",
      "Epoch 795 \tTraining Loss: 0.21495876848852366 \t Validation Loss: 0.00580488920211792\n",
      "Epoch 796 \tTraining Loss: 0.21154097647502504 \t Validation Loss: 0.006004086512106436\n",
      "Epoch 797 \tTraining Loss: 0.2115807463150799 \t Validation Loss: 0.0060956619403980394\n",
      "Epoch 798 \tTraining Loss: 0.21217623529807259 \t Validation Loss: 0.005602233498184769\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_model(train_dl, test_dl, model, epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
